1
00:00:02,690 --> 00:00:03,690
Greetings everyone.

2
00:00:03,690 --> 00:00:07,690
Welcome to the most beginner-friendly and
yet the most advanced and up-to-date Stable

3
00:00:07,690 --> 00:00:09,610
Diffusion DreamBooth model training tutorial.

4
00:00:09,610 --> 00:00:14,990
In this guide video, I am going to use the
latest Automatic1111 web UI and the DreamBooth

5
00:00:14,990 --> 00:00:16,170
extension.

6
00:00:16,170 --> 00:00:20,421
The interface and the features of the DreamBooth
plugin have been significantly changed, so

7
00:00:20,421 --> 00:00:23,600
all other tutorials are now obsolete.

8
00:00:23,600 --> 00:00:27,609
I have been experimenting for over 7 days
to find the best settings and the training

9
00:00:27,609 --> 00:00:28,609
parameters.

10
00:00:28,609 --> 00:00:33,829
Moreover, I tried to learn what each option
does and I have explained everything in this

11
00:00:33,829 --> 00:00:34,870
video.

12
00:00:34,870 --> 00:00:37,950
Before starting, let me provide some quick
info.

13
00:00:37,950 --> 00:00:42,510
Stable Diffusion is a text-to-image generative
public AI model, and the Automatic1111 web

14
00:00:42,510 --> 00:00:47,690
UI is a tool developed by the open source
community to use Stable Diffusion easily.

15
00:00:47,690 --> 00:00:52,219
DreamBooth is an AI algorithm that allows
you to teach new subjects or even styles to

16
00:00:52,219 --> 00:00:57,320
existing Stable Diffusion models very successfully,
such as teaching the face of a person.

17
00:00:57,320 --> 00:01:02,480
In this tutorial, I am going to use freshly
installed Automatic1111 web UI to teach my

18
00:01:02,480 --> 00:01:05,820
face by using Stable Diffusion 1.5 official
version.

19
00:01:05,820 --> 00:01:11,560
I will also show how you can do the same training
on Stable Diffusion version 2.1 as well.

20
00:01:11,560 --> 00:01:16,990
Moreover, I will show you how you can inject
your trained subject, in this case my face,

21
00:01:16,990 --> 00:01:19,990
into any custom model and obtain amazing results.

22
00:01:19,990 --> 00:01:24,579
I will demonstrate an example by using the
very popular and very high-quality custom

23
00:01:24,579 --> 00:01:26,540
model Protogen x3.4.

24
00:01:26,540 --> 00:01:32,299
With this injection methodology, you can use
any namely released custom model and obtain

25
00:01:32,299 --> 00:01:33,620
even better results.

26
00:01:33,620 --> 00:01:37,250
You won't even need to retrain your subject
for this to work.

27
00:01:37,250 --> 00:01:42,200
This method provides such high-quality images
that you cannot even obtain them on paid services

28
00:01:42,200 --> 00:01:44,659
like Lensa or Midjourney.

29
00:01:44,659 --> 00:01:49,380
The Automatic1111 web UI is getting constantly
updated, so let me show you the version I

30
00:01:49,380 --> 00:01:53,500
am using from official repository.

31
00:01:53,500 --> 00:01:57,070
This is the official repository of the Stable
Diffusion web UI.

32
00:01:57,070 --> 00:02:00,030
It has been recently taken down, but it is
now back again.

33
00:02:00,030 --> 00:02:05,149
So if you can't find this URL, just check
out the video and I will update the description

34
00:02:05,149 --> 00:02:11,180
of the video and the comment of the video
so you will find the latest link of the Automatic1111.

35
00:02:11,180 --> 00:02:20,190
So the commit we are using is published 9
hours ago, January 7, 2023.

36
00:02:20,190 --> 00:02:25,129
If you don't know how to install Automatic1111
web UI, I have a great tutorial for that.

37
00:02:25,129 --> 00:02:27,960
So this is the homepage of our YouTube channel.

38
00:02:27,960 --> 00:02:34,000
Go to playlist and in here you will see Stable
Diffusion DreamBooth playlist and in this

39
00:02:34,000 --> 00:02:38,740
playlist, easiest ways to install and run
Stable Diffusion web UI on PC.

40
00:02:38,740 --> 00:02:43,629
I will put the link of this video to the description
and also you can watch how to use Stable Diffusion

41
00:02:43,629 --> 00:02:46,420
version 2.1 and different models in the web
UI.

42
00:02:46,420 --> 00:02:47,980
This is also very important.

43
00:02:47,980 --> 00:02:51,769
I will also put the link of this video to
the description as well.

44
00:02:51,769 --> 00:02:52,980
One more thing.

45
00:02:52,980 --> 00:02:54,470
This is commonly asked.

46
00:02:54,470 --> 00:03:00,599
If you encounter any problem, go to about
page of our channel and in here you will see

47
00:03:00,599 --> 00:03:01,739
our Discord channel link.

48
00:03:01,739 --> 00:03:03,760
As you can see, I am currently hovering that.

49
00:03:03,760 --> 00:03:08,400
You can join our Discord channel and ask me
any questions that you encounter.

50
00:03:08,400 --> 00:03:11,920
So this is our beginning screen of the Stable
Diffusion.

51
00:03:11,920 --> 00:03:14,780
And first let's start with installing our
extension, DreamBooth.

52
00:03:14,780 --> 00:03:21,860
To do that, go to extensions tab, click available
load from and in here you will see DreamBooth

53
00:03:21,860 --> 00:03:23,000
extension.

54
00:03:23,000 --> 00:03:26,349
When you type DreamBooth, it is listed in
here.

55
00:03:26,349 --> 00:03:29,920
I am just clicking install and it is getting
installed.

56
00:03:29,920 --> 00:03:35,750
You should see a message here: OK, it has
been installed.

57
00:03:35,750 --> 00:03:38,330
We have one error, but it is not a problem.

58
00:03:38,330 --> 00:03:39,890
It still works.

59
00:03:39,890 --> 00:03:45,440
So you see, we have a message on CMD window
and also installed into the C web UI tutorial

60
00:03:45,440 --> 00:03:47,760
extensions as the DreamBooth extension.

61
00:03:47,760 --> 00:03:53,140
Now we have to restart CMD window because
we are the first time installing and it is

62
00:03:53,140 --> 00:03:54,150
a necessity.

63
00:03:54,150 --> 00:03:56,299
Otherwise it won't work.

64
00:03:56,299 --> 00:03:58,230
Let's close.

65
00:03:58,230 --> 00:03:59,670
Let's restart.

66
00:03:59,670 --> 00:04:02,470
OK, restart has been completed.

67
00:04:02,470 --> 00:04:09,170
Let's just refresh and then go back to extensions
and check for updates every time you start.

68
00:04:09,170 --> 00:04:12,020
OK, it has been just got updated.

69
00:04:12,020 --> 00:04:14,950
So I'm just clicking apply and restart UI.

70
00:04:14,950 --> 00:04:16,760
OK, it is done.

71
00:04:16,760 --> 00:04:19,630
After the first time installation.

72
00:04:19,630 --> 00:04:23,040
You don't need to restart CMD window once
again.

73
00:04:23,040 --> 00:04:26,389
So you see, this is how frequently this stuff
are getting updated.

74
00:04:26,389 --> 00:04:30,820
Literally, it has been updated just now, as
you can see.

75
00:04:30,820 --> 00:04:33,340
So you should always check the latest version.

76
00:04:33,340 --> 00:04:35,570
Now we can start our tutorial.

77
00:04:35,570 --> 00:04:36,570
We are now.

78
00:04:36,570 --> 00:04:38,330
We see the DreamBooth tab in the interface.

79
00:04:38,330 --> 00:04:39,740
We click that.

80
00:04:39,740 --> 00:04:44,910
This is the interface where you are going
to generate our model and train our face or

81
00:04:44,910 --> 00:04:46,509
new subject.

82
00:04:46,509 --> 00:04:51,410
First of all, we need to generate our model.

83
00:04:51,410 --> 00:04:53,310
You can simply enter any name here.

84
00:04:53,310 --> 00:04:54,310
It doesn't matter.

85
00:04:54,310 --> 00:05:02,580
So I will enter as web UI and the identifier
prompt of my model, which will be ohwx.

86
00:05:02,580 --> 00:05:05,949
I will explain why it will be ohwx.

87
00:05:05,949 --> 00:05:08,520
Then we need to check source point.

88
00:05:08,520 --> 00:05:12,570
You can also import from Hugging Face, but
I don't suggest that it is not necessary.

89
00:05:12,570 --> 00:05:17,090
I am checking version 1.5 Pruned ckpt.

90
00:05:17,090 --> 00:05:24,110
So version 1.5 pruned ckpt available in the
official repository of StableDiffusion 1.5.

91
00:05:24,110 --> 00:05:26,020
You can just download it from here.

92
00:05:26,020 --> 00:05:33,150
Why we are using Pruned ckpt, not the pruned-emaonly
ckpt, because this is better for training

93
00:05:33,150 --> 00:05:34,229
new subjects.

94
00:05:34,229 --> 00:05:39,080
When you click here, you can just download
it with clicking here.

95
00:05:39,080 --> 00:05:44,169
And after you put that into your model folder,
it will be also available here, as you can

96
00:05:44,169 --> 00:05:46,130
see.

97
00:05:46,130 --> 00:05:48,080
OK.

98
00:05:48,080 --> 00:05:51,010
Then just click the create model button.

99
00:05:51,010 --> 00:05:54,490
OK, you see.

100
00:05:54,490 --> 00:05:59,070
We have a message checkpoint successfully
extracted to this folder.

101
00:05:59,070 --> 00:06:00,110
Where it is.

102
00:06:00,110 --> 00:06:01,360
Let me show you.

103
00:06:01,360 --> 00:06:04,500
It is inside Web UI Tutorial.

104
00:06:04,500 --> 00:06:11,300
And let's go to our models and inside DreamBooth,
inside Web UI ohwx and in here working.

105
00:06:11,300 --> 00:06:17,810
And these are actually weights of the model
that we have just composed.

106
00:06:17,810 --> 00:06:19,349
Lets continue.

107
00:06:19,349 --> 00:06:21,840
Now this model is selected here.

108
00:06:21,840 --> 00:06:24,550
This is where the selection we make.

109
00:06:24,550 --> 00:06:28,639
After we make this selection, we will train
the selected model.

110
00:06:28,639 --> 00:06:29,639
Yes.

111
00:06:29,639 --> 00:06:30,639
OK.

112
00:06:30,639 --> 00:06:33,200
Now let's go to the settings tab in here.

113
00:06:33,200 --> 00:06:35,160
First click performance wizard.

114
00:06:35,160 --> 00:06:39,900
It will set the parameters according to the
VRAM of your GPU.

115
00:06:39,900 --> 00:06:44,789
If you have less than 12GB of GPU, it is really
hard to use DreamBooth.

116
00:06:44,789 --> 00:06:45,789
Unfortunately.

117
00:06:45,789 --> 00:06:48,750
You can use LoRA, but it is a topic of another
video.

118
00:06:48,750 --> 00:06:52,270
Actually, it is almost same as this video,
but there is.

119
00:06:52,270 --> 00:06:56,900
There are some just few tricks, and I already
have a video for LoRA.

120
00:06:56,900 --> 00:07:00,840
So after watching this video, if you watch
that video, the LoRA video.

121
00:07:00,840 --> 00:07:05,130
You can easily apply LoRA to your training.

122
00:07:05,130 --> 00:07:06,130
It is in here.

123
00:07:06,130 --> 00:07:08,530
You see how to do Stable Diffusion LoRA training
by using.

124
00:07:08,530 --> 00:07:13,160
I will also put the link of this video to
the description as well.

125
00:07:13,160 --> 00:07:16,449
So training steps per image epochs.

126
00:07:16,449 --> 00:07:19,000
First of all, let me explain what is epoch.

127
00:07:19,000 --> 00:07:26,009
We will have a training data set, the pictures
of the subject that we are going to teach.

128
00:07:26,009 --> 00:07:29,860
In this case, I am going to teach myself.

129
00:07:29,860 --> 00:07:33,280
I will use 12 images of myself.

130
00:07:33,280 --> 00:07:38,090
Therefore, one epoch means that 12 steps.

131
00:07:38,090 --> 00:07:44,280
So each step is a training step and each epoch
is training all of the training images one

132
00:07:44,280 --> 00:07:45,280
time.

133
00:07:45,280 --> 00:07:50,699
So one epoch means 12 steps in my case, because
I have to have training images.

134
00:07:50,699 --> 00:07:52,490
And how many epochs we want?

135
00:07:52,490 --> 00:07:57,169
For teaching faces it is usually suggested
150.

136
00:07:57,169 --> 00:08:00,690
So when you go to the concepts, just click
training with a person.

137
00:08:00,690 --> 00:08:05,490
It will set the most appropriate values for
person.

138
00:08:05,490 --> 00:08:07,699
So you see, now it is set to 150.

139
00:08:07,699 --> 00:08:13,220
However, you can set this as much as you want
and you can use a certain checkpoints.

140
00:08:13,220 --> 00:08:14,220
I will explain that.

141
00:08:14,220 --> 00:08:17,270
So I'm just going to make it 300.

142
00:08:17,270 --> 00:08:22,379
And how much time you want to wait between
each epoch: zero, This is also zero.

143
00:08:22,379 --> 00:08:24,020
OK, this is important.

144
00:08:24,020 --> 00:08:29,110
How frequently we want to save our training.

145
00:08:29,110 --> 00:08:34,719
You know, if your computer crashes, if you
cancel your training, if whatever happens,

146
00:08:34,719 --> 00:08:40,380
you will be able to continue from your latest
saved model.

147
00:08:40,380 --> 00:08:42,520
Therefore, this is important.

148
00:08:42,520 --> 00:08:48,100
Also, if you do over training and you want
to use previous training checkpoint, you also

149
00:08:48,100 --> 00:08:49,100
need to have a save.

150
00:08:49,100 --> 00:08:50,720
So I'm going to set this as 10.

151
00:08:50,720 --> 00:08:55,910
Be careful that when you are doing the DreamBooth
training, it is usually taking about 4 to

152
00:08:55,910 --> 00:08:58,279
5 gigabytes for per saving.

153
00:08:58,279 --> 00:09:04,430
So if you don't have much hard drive space,
you need to set this a higher number perhaps.

154
00:09:04,430 --> 00:09:08,800
This is saving preview images each epoch,
for example, or for whatever the number of

155
00:09:08,800 --> 00:09:09,800
epochs you want.

156
00:09:09,800 --> 00:09:13,980
This doesn't take space, but this will slow
you down.

157
00:09:13,980 --> 00:09:17,480
So I'm just going to leave this as five.

158
00:09:17,480 --> 00:09:19,580
Batch size: Now, this is very important.

159
00:09:19,580 --> 00:09:23,500
If you increase batch size, it will speed
up your training significantly.

160
00:09:23,500 --> 00:09:29,600
However, this will also increase your GPU
memory usage significantly as well.

161
00:09:29,600 --> 00:09:35,610
If you increase these numbers, you need to
increase both of them equally to obtain the

162
00:09:35,610 --> 00:09:36,610
best results.

163
00:09:36,610 --> 00:09:40,420
So now, for example, it will be almost four
times faster.

164
00:09:40,420 --> 00:09:45,649
Also, make sure that your training images
count is divisible to this number.

165
00:09:45,649 --> 00:09:54,800
So two multiplied by two makes four and you
must have training number of images divisible

166
00:09:54,800 --> 00:09:55,800
to four.

167
00:09:55,800 --> 00:10:02,130
So it can be four images, eight images, 12
images, 16 images, 20 images, but it shouldn't

168
00:10:02,130 --> 00:10:03,570
be 17 images.

169
00:10:03,570 --> 00:10:06,360
OK, this is the formula.

170
00:10:06,360 --> 00:10:12,550
Let's say you have 16 gigabytes of GPU RAM,
then you can make this three by three.

171
00:10:12,550 --> 00:10:17,610
And then you should have nine or 18 or 27
or 36 images.

172
00:10:17,610 --> 00:10:18,760
That is the formula.

173
00:10:18,760 --> 00:10:22,529
I'm just going to leave this one by one for
now.

174
00:10:22,529 --> 00:10:28,380
Also, another thing is, if you make this two
and two, like this, it will be four times.

175
00:10:28,380 --> 00:10:35,010
Then you need to also increase learning rate
by four times, like this and this.

176
00:10:35,010 --> 00:10:36,480
Otherwise it will be very slow.

177
00:10:36,480 --> 00:10:40,360
It is also requiring to speed up the learning
rate as well.

178
00:10:40,360 --> 00:10:42,880
As much as you increase them.

179
00:10:42,880 --> 00:10:46,800
Since I will use one by one, I am just going
to leave the default learning rate.

180
00:10:46,800 --> 00:10:50,730
OK, set gradients to none when zeroing.

181
00:10:50,730 --> 00:10:55,899
If you select this, it will increase the GPU
RAM usage.

182
00:10:55,899 --> 00:10:57,990
How can you know that?

183
00:10:57,990 --> 00:11:03,970
The DreamBooth has a wiki pages and in here
they have RAM usage settings.

184
00:11:03,970 --> 00:11:06,000
Let me show you.

185
00:11:06,000 --> 00:11:10,470
OK in here: settings known to use more VRAM.

186
00:11:10,470 --> 00:11:12,420
High batch size, as I just explained.

187
00:11:12,420 --> 00:11:18,540
Setting gradients to none when zeroing, which
is these settings in here.

188
00:11:18,540 --> 00:11:23,420
So when you check this, it will use more VRAM
and then use EMA.

189
00:11:23,420 --> 00:11:24,420
OK.

190
00:11:24,420 --> 00:11:25,420
Now let's continue.

191
00:11:25,420 --> 00:11:26,420
And I will explain.

192
00:11:26,420 --> 00:11:31,070
Gradient checkpoint: This is technique to
reduce memory usage by clearing activations.

193
00:11:31,070 --> 00:11:35,440
So it is good to check it out.

194
00:11:35,440 --> 00:11:37,959
And then we are not just passing here.

195
00:11:37,959 --> 00:11:42,230
These are just kind of more advanced things
to play with it.

196
00:11:42,230 --> 00:11:47,529
After you get used to how to use the DreamBooth,
you can just change them, but in the learning

197
00:11:47,529 --> 00:11:49,910
stage just leave them as they are.

198
00:11:49,910 --> 00:11:53,920
If you set these too high, it will get too
fast trained.

199
00:11:53,920 --> 00:11:56,670
However, it will also over train easily.

200
00:11:56,670 --> 00:12:01,220
If you get them too low, then you may never
get it trained.

201
00:12:01,220 --> 00:12:05,910
So this is kind of experimental thing that
you need to do a lot of experimentation.

202
00:12:05,910 --> 00:12:07,220
Image processing and resolution.

203
00:12:07,220 --> 00:12:08,220
This is important.

204
00:12:08,220 --> 00:12:18,620
When you use a model version based on the
version 1.X, then they are 512 pixels.

205
00:12:18,620 --> 00:12:24,459
If you use version 2.1 then there is also
768 pixels version.

206
00:12:24,459 --> 00:12:28,880
So you need to set this according to the version
of your base model.

207
00:12:28,880 --> 00:12:30,970
OK, the base model, the source checkpoint.

208
00:12:30,970 --> 00:12:32,209
We checked here.

209
00:12:32,209 --> 00:12:35,949
Since we are using version 1.5, official version.

210
00:12:35,949 --> 00:12:39,410
It is 512 pixels.

211
00:12:39,410 --> 00:12:40,790
Don't apply horizontal flip.

212
00:12:40,790 --> 00:12:42,660
This is not good for faces.

213
00:12:42,660 --> 00:12:44,149
Center crop.

214
00:12:44,149 --> 00:12:46,990
If your images are not cropped, you should
check this out.

215
00:12:46,990 --> 00:12:49,940
I will explain how to set your images.

216
00:12:49,940 --> 00:12:53,640
Since my images are center cropped, I am not
checking this.

217
00:12:53,640 --> 00:12:54,640
Sanity sample prompt.

218
00:12:54,640 --> 00:12:55,700
OK, this is important.

219
00:12:55,700 --> 00:13:02,959
We are going to use this prompt to see the
overall training of the model.

220
00:13:02,959 --> 00:13:07,200
But how, in in terms of overtraining or not.

221
00:13:07,200 --> 00:13:11,040
During the training training, I will explain.

222
00:13:11,040 --> 00:13:16,350
So I am going to enter here photo of ohwx
man by Tomer Hanuka.

223
00:13:16,350 --> 00:13:20,050
I will explain why did I enter this prompt.

224
00:13:20,050 --> 00:13:22,130
And by Tomer Hanuka.

225
00:13:22,130 --> 00:13:23,360
You will understand it.

226
00:13:23,360 --> 00:13:26,040
Miscellaneous, pre trained VAE or path.

227
00:13:26,040 --> 00:13:28,480
These are advanced things and you don't need
currently.

228
00:13:28,480 --> 00:13:29,480
OK.

229
00:13:29,480 --> 00:13:31,529
OK, advanced stuff.

230
00:13:31,529 --> 00:13:33,589
This is important.

231
00:13:33,589 --> 00:13:38,010
If you check box the use EMA, then it will
improve your training quality.

232
00:13:38,010 --> 00:13:41,100
However, it also increases the RAM usage significantly.

233
00:13:41,100 --> 00:13:45,880
Use eight bit Adam: This will reduce the RAM
usage.

234
00:13:45,880 --> 00:13:47,790
BF16: This is also.

235
00:13:47,790 --> 00:13:50,820
This will also reduce RAM usage.

236
00:13:50,820 --> 00:13:54,870
xFormers: This will significantly increase
your training speed.

237
00:13:54,870 --> 00:13:59,740
Cache Latent: This will also reduce the VRAM
usage.

238
00:13:59,740 --> 00:14:02,759
All of these are actually written in this
page.

239
00:14:02,759 --> 00:14:05,800
The out of memory top of the wiki.

240
00:14:05,800 --> 00:14:08,860
I will put this into the description.

241
00:14:08,860 --> 00:14:11,949
So you see these are all decreasing the RAM
usage.

242
00:14:11,949 --> 00:14:19,329
Actually, it says that cache Latent increases,
but as far as I know this is not increasing.

243
00:14:19,329 --> 00:14:22,110
But you can test that.

244
00:14:22,110 --> 00:14:25,019
So the Step Ratio of Text Encoder Training.

245
00:14:25,019 --> 00:14:27,700
This will improve your training quality.

246
00:14:27,700 --> 00:14:31,730
However, it will also increase the RAM usage
of the graphic card.

247
00:14:31,730 --> 00:14:37,009
So if you encounter out of memory error, you
should set this zero.

248
00:14:37,009 --> 00:14:44,120
But the optimal value for faces is 0.7, for
style 0.2.

249
00:14:44,120 --> 00:14:46,410
And the other things you don't need to play
with them.

250
00:14:46,410 --> 00:14:49,240
They are more advanced stuff.

251
00:14:49,240 --> 00:14:51,800
OK, now the concepts.

252
00:14:51,800 --> 00:14:54,959
This is the very important part.

253
00:14:54,959 --> 00:14:59,970
You can set a [filewords], prompts and directories.

254
00:14:59,970 --> 00:15:03,390
So first of all we have to set our training
data set.

255
00:15:03,390 --> 00:15:04,850
Training data set directory.

256
00:15:04,850 --> 00:15:07,970
Where are my training data set?

257
00:15:07,970 --> 00:15:12,579
It is inside my pictures folder and it is
in here Best DB.

258
00:15:12,579 --> 00:15:19,389
So all of these images are now 512 by 512
pixels.

259
00:15:19,389 --> 00:15:21,130
Let me show their original version.

260
00:15:21,130 --> 00:15:26,050
So their original version is here.

261
00:15:26,050 --> 00:15:27,529
How did I set them like this?

262
00:15:27,529 --> 00:15:31,759
I have used a Paint .NET to crop them as I
want.

263
00:15:31,759 --> 00:15:32,759
For example.

264
00:15:32,759 --> 00:15:36,060
Let me show you: paint dot net is a free tool,
by the way.

265
00:15:36,060 --> 00:15:40,769
You can install it from the Google.

266
00:15:40,769 --> 00:15:46,589
Just click, like this, and then I am just
cropping them with a square.

267
00:15:46,589 --> 00:15:52,149
So I click rectangle, select, then click here,
then in here, fixed ratio like this, Then

268
00:15:52,149 --> 00:15:55,769
you can pick the any part of the image you
want.

269
00:15:55,769 --> 00:15:57,230
Just for example here.

270
00:15:57,230 --> 00:16:01,910
Then you can control-C control-N and it will
paste into a new place.

271
00:16:01,910 --> 00:16:03,220
You can save it.

272
00:16:03,220 --> 00:16:04,220
Or in here.

273
00:16:04,220 --> 00:16:10,730
You can just resize these to very low resolution
like this, with control-R. It will open resize

274
00:16:10,730 --> 00:16:13,870
type like this, then control-V and expand.

275
00:16:13,870 --> 00:16:15,740
You see, now it is cropped.

276
00:16:15,740 --> 00:16:17,660
Alternatively, you can use Birme .NET.

277
00:16:17,660 --> 00:16:22,550
Birme dot net is a famous site to crop images.

278
00:16:22,550 --> 00:16:25,069
It is commonly used in the community.

279
00:16:25,069 --> 00:16:30,170
You can just, for example, upload any image
there and crop them.

280
00:16:30,170 --> 00:16:33,660
For example, let's upload this image.

281
00:16:33,660 --> 00:16:37,490
These are currently squared, but if they are
not square, it will also automatically let

282
00:16:37,490 --> 00:16:38,490
you square them.

283
00:16:38,490 --> 00:16:42,769
Let me show: OK, you see, both of these images
are not cropped.

284
00:16:42,769 --> 00:16:48,261
So you are able to crop them with your mouse
like this: set the position, then set the

285
00:16:48,261 --> 00:16:51,040
resolution from here: 512, 512.

286
00:16:51,040 --> 00:16:55,750
If you use SD version 2.1, then they will
be seven 768 pixels.

287
00:16:55,750 --> 00:16:58,790
OK, you can also use auto detect image focal
point.

288
00:16:58,790 --> 00:16:59,790
Do not resize.

289
00:16:59,790 --> 00:17:01,380
And you can click here.

290
00:17:01,380 --> 00:17:03,000
If you check, do not resize, It won't.

291
00:17:03,000 --> 00:17:06,030
They won't be resized to this resolution.

292
00:17:06,030 --> 00:17:08,709
Then save a zip and all of them will be saved
as zip.

293
00:17:08,709 --> 00:17:12,510
Then you can extract them with the software
you have.

294
00:17:12,510 --> 00:17:17,000
If you don't have any software like Winrar
Windows still able to extract them.

295
00:17:17,000 --> 00:17:18,040
All right.

296
00:17:18,040 --> 00:17:22,140
If you can't make them, just join Discord
and I will help you, hopefully.

297
00:17:22,140 --> 00:17:23,939
So data set directory.

298
00:17:23,939 --> 00:17:27,119
When you ready your images, then we will enter
the path of it.

299
00:17:27,119 --> 00:17:28,900
So this is my.

300
00:17:28,900 --> 00:17:32,100
Let me enter the folder directory.

301
00:17:32,100 --> 00:17:34,929
I click here and you see I am able to select
the path.

302
00:17:34,929 --> 00:17:38,650
I do control-C to copy it, paste it here (ctrl-v).

303
00:17:38,650 --> 00:17:43,440
So this is the directory where my training
images are located.

304
00:17:43,440 --> 00:17:46,800
Classification directory: Now, what is classification?

305
00:17:46,800 --> 00:17:54,190
Classification are generic images that we
will use to not over train our model and also

306
00:17:54,190 --> 00:17:58,450
keep the inner sanity of the model.

307
00:17:58,450 --> 00:18:02,530
So that the entire model does not become looking
like us.

308
00:18:02,530 --> 00:18:03,530
OK.

309
00:18:03,530 --> 00:18:06,700
So for this I will just generate a new folder.

310
00:18:06,700 --> 00:18:10,190
Yes, I have copy pasted the path.

311
00:18:10,190 --> 00:18:13,520
I will set it as web UI tutorial.

312
00:18:13,520 --> 00:18:16,950
You can also enter an existing another directory.

313
00:18:16,950 --> 00:18:18,010
It is fine.

314
00:18:18,010 --> 00:18:25,620
Instance token: Now [filewords] are used to
set the different description for each training

315
00:18:25,620 --> 00:18:26,620
images.

316
00:18:26,620 --> 00:18:29,620
This is very, very advanced and hard to do.

317
00:18:29,620 --> 00:18:34,290
So I will explain this in the later parts
of the tutorial video.

318
00:18:34,290 --> 00:18:35,990
For now I will just skip them.

319
00:18:35,990 --> 00:18:42,380
You can also skip to that part in the video,
because I will put the sections of the video

320
00:18:42,380 --> 00:18:43,380
into the description.

321
00:18:43,380 --> 00:18:46,500
Now prompts: This is very important.

322
00:18:46,500 --> 00:18:53,840
The instance prompt is used to define the
keyword that will activate our new subject

323
00:18:53,840 --> 00:18:56,440
that we taught to the model.

324
00:18:56,440 --> 00:19:05,200
So in here you have to pick a unique word,
but it has to be very specific and rare.

325
00:19:05,200 --> 00:19:07,179
Whatever you enter to the model.

326
00:19:07,179 --> 00:19:10,110
They will get turned into tokens.

327
00:19:10,110 --> 00:19:11,900
They will split into tokens.

328
00:19:11,900 --> 00:19:15,280
So there is a reddit thread that explains
the rare tokens.

329
00:19:15,280 --> 00:19:24,320
I will put link of this page to the description
and in here the rarity of the tokens are listed.

330
00:19:24,320 --> 00:19:29,650
So, for example, you have entered, let's say,
mill.

331
00:19:29,650 --> 00:19:34,370
It is a single token, but mill probably exist
in the real life a lot.

332
00:19:34,370 --> 00:19:40,750
Therefore, you have to go to the bottom and
try to find rare tokens that you can't make

333
00:19:40,750 --> 00:19:42,200
sense of.

334
00:19:42,200 --> 00:19:43,940
For example, they.

335
00:19:43,940 --> 00:19:49,280
Also, these tokens should be used in other
languages as well.

336
00:19:49,280 --> 00:19:57,620
For example, from here: ohwx is a very famous
token because this is a token that almost

337
00:19:57,620 --> 00:19:59,960
does not exist in anywhere.

338
00:19:59,960 --> 00:20:06,380
When I type ohwx to the Google, you see all
unrelated things.

339
00:20:06,380 --> 00:20:07,760
They look like spam.

340
00:20:07,760 --> 00:20:14,020
So this is a good token and, for example,
you can also try other tokens here that looks

341
00:20:14,020 --> 00:20:15,030
like to you weird.

342
00:20:15,030 --> 00:20:16,140
Maybe this one?

343
00:20:16,140 --> 00:20:17,250
Yes, this, OK.

344
00:20:17,250 --> 00:20:27,049
I'm not sure if this is a real name or not,
so you can verify it, but ohwx works very

345
00:20:27,049 --> 00:20:31,190
well and the token you pick is extremely important.

346
00:20:31,190 --> 00:20:37,950
Because your training will begin from that
token and you can inject a new token that

347
00:20:37,950 --> 00:20:44,350
does not exist in the database, so everything
you enter will become a token that it knows

348
00:20:44,350 --> 00:20:46,110
they will get splint into.

349
00:20:46,110 --> 00:20:54,520
Even if you generate a new keyword, such as
SECourses, the model will not see this as

350
00:20:54,520 --> 00:20:55,780
an SECourses.

351
00:20:55,780 --> 00:20:57,020
How will it see it?

352
00:20:57,020 --> 00:21:01,440
First it will look to S key, SE key.

353
00:21:01,440 --> 00:21:03,890
So the SE key does exist, OK.

354
00:21:03,890 --> 00:21:05,830
Then it will look sec.

355
00:21:05,830 --> 00:21:10,970
So, yes, sec also exists.

356
00:21:10,970 --> 00:21:12,250
And then it will look seco.

357
00:21:12,250 --> 00:21:16,380
OK, there is no seco, so it will get split
into sec.

358
00:21:16,380 --> 00:21:23,960
And then it will be like it will check the
other characters, the remaining characters,

359
00:21:23,960 --> 00:21:33,990
so they will all get split into yes our SECourses
will probably become sec our ses or something

360
00:21:33,990 --> 00:21:34,990
like that.

361
00:21:34,990 --> 00:21:35,990
You see, you are understanding.

362
00:21:35,990 --> 00:21:37,190
I am hoping that.

363
00:21:37,190 --> 00:21:44,650
So the keyword you enter will get split into
tokens, no matter what you enter.

364
00:21:44,650 --> 00:21:51,510
Therefore, we are picking a single token that
is very rare from this list and I have done

365
00:21:51,510 --> 00:21:52,510
many tests.

366
00:21:52,510 --> 00:21:58,990
So ohwx is working very well and then we need
to enter the class of the subject we are going

367
00:21:58,990 --> 00:21:59,990
to teach.

368
00:21:59,990 --> 00:22:00,990
What am I going to teach?

369
00:22:00,990 --> 00:22:03,450
I am going to teach the face of me.

370
00:22:03,450 --> 00:22:04,490
So it's the face of man.

371
00:22:04,490 --> 00:22:07,309
Therefore, I am just entering man.

372
00:22:07,309 --> 00:22:09,400
So this is really important.

373
00:22:09,400 --> 00:22:15,230
It will use the underlying knowledge of man
in the model to learn my face.

374
00:22:15,230 --> 00:22:21,970
Class prompt: now, as I said, this will be
used to keep sanity of our model and prevent

375
00:22:21,970 --> 00:22:22,970
overtraining.

376
00:22:22,970 --> 00:22:27,230
When you also hover it, it says: read me for
more info.

377
00:22:27,230 --> 00:22:31,650
I wonder if they added into the wiki yet.

378
00:22:31,650 --> 00:22:33,780
In the basics perhaps?

379
00:22:33,780 --> 00:22:38,950
OK, in the wiki, in the basics they have a
small explanation.

380
00:22:38,950 --> 00:22:44,840
A class specific prior preservation loss is
also introduced to prevent overfitting and

381
00:22:44,840 --> 00:22:49,340
encourage the generation of diverse instances
of the same class.

382
00:22:49,340 --> 00:22:51,700
They have made an example like this.

383
00:22:51,700 --> 00:22:55,510
So in class prompt I am going to enter photo
of man.

384
00:22:55,510 --> 00:23:00,110
OK, you see, these two are same and the sample
prompt.

385
00:23:00,110 --> 00:23:04,770
This will be used to generate preview images
during the training so we will be able to

386
00:23:04,770 --> 00:23:11,010
see how the training is going on and if it
is becoming too overtrained or not.

387
00:23:11,010 --> 00:23:15,210
So in here I am going to enter photo of ohwx
man.

388
00:23:15,210 --> 00:23:22,510
OK, I am not entering any negative prompts
and I'm not using any sample prompt template.

389
00:23:22,510 --> 00:23:28,250
So these are more, let's say, advanced things
that you can also play with them after you

390
00:23:28,250 --> 00:23:30,400
learned the basics.

391
00:23:30,400 --> 00:23:32,870
And in here, class images per instance.

392
00:23:32,870 --> 00:23:39,799
In the community it is usually said that have
minimal 300 images total.

393
00:23:39,799 --> 00:23:42,710
In the official paper of the DreamBooth.

394
00:23:42,710 --> 00:23:45,050
Which is here.

395
00:23:45,050 --> 00:23:48,990
I will also put the link of this paper to
the description.

396
00:23:48,990 --> 00:23:53,370
They have used 200 classification images.

397
00:23:53,370 --> 00:23:59,850
I have made some tests but I can't say for
sure how much minimum is necessary.

398
00:23:59,850 --> 00:24:06,570
So I am just going to follow the community
and to reach the 300 images I need to enter,

399
00:24:06,570 --> 00:24:12,020
let's easily calculate 300 divided by the
number of training images.

400
00:24:12,020 --> 00:24:14,260
I have 12, so 25..

401
00:24:14,260 --> 00:24:16,190
You can also calculate like this.

402
00:24:16,190 --> 00:24:18,180
So classification, CVG scale.

403
00:24:18,180 --> 00:24:25,410
This is same as text2images CFG scale how
many, how much CFG scale you want to use for

404
00:24:25,410 --> 00:24:27,850
generating classification images?

405
00:24:27,850 --> 00:24:32,580
By the way, you can also use text2image tab
to generate your classification images.

406
00:24:32,580 --> 00:24:35,860
Put them into the folder that we set here.

407
00:24:35,860 --> 00:24:40,730
Then the extension will not generate any new
images.

408
00:24:40,730 --> 00:24:41,730
It is up to you.

409
00:24:41,730 --> 00:24:47,570
You can use the both ways, but if you use
this way, it will also generate a text description

410
00:24:47,570 --> 00:24:54,700
file same as the image name, and it will put
the description you have typed here inside.

411
00:24:54,700 --> 00:24:56,640
That I will show in a moment.

412
00:24:56,640 --> 00:24:58,270
Classification steps.

413
00:24:58,270 --> 00:25:01,490
So this is the number of steps equal to the
in here.

414
00:25:01,490 --> 00:25:05,630
Sampling steps: OK, and number of samples
to generate.

415
00:25:05,630 --> 00:25:10,990
So this is the number of samples that we want
to be generated during the training to see

416
00:25:10,990 --> 00:25:12,770
how the training is going on.

417
00:25:12,770 --> 00:25:15,659
You can set this to 1, 2, 3, 4, whatever you
want.

418
00:25:15,659 --> 00:25:17,159
Sample seed -1.

419
00:25:17,159 --> 00:25:23,500
It means that the every image generated for
samples will be different random with a random

420
00:25:23,500 --> 00:25:27,960
seed and the samples CFG scale 7.5.

421
00:25:27,960 --> 00:25:30,409
You don't need to change this.

422
00:25:30,409 --> 00:25:32,460
These are just same as the text2image.

423
00:25:32,460 --> 00:25:36,620
You will make sense of it after you get used
to text to image.

424
00:25:36,620 --> 00:25:45,399
OK, and now let's return back here: How many
images we want to generate for classification

425
00:25:45,399 --> 00:25:47,440
at the same time in parallel.

426
00:25:47,440 --> 00:25:50,340
So I have 12 GB VRAM memory.

427
00:25:50,340 --> 00:25:56,360
Therefore, I am able to generate 10 images
as a batches, so it will take lesser time

428
00:25:56,360 --> 00:25:58,080
to generate classification images.

429
00:25:58,080 --> 00:26:06,250
By the way, you only need to generate classification
images one time for each class prompt.

430
00:26:06,250 --> 00:26:11,659
So if you don't change photo of man, if you
don't change your subject class, then you

431
00:26:11,659 --> 00:26:14,279
you don't need to generate them once again.

432
00:26:14,279 --> 00:26:20,380
So for showing you, I will just set this as
five and you will understand.

433
00:26:20,380 --> 00:26:24,170
It will generate images, five and five as
batches.

434
00:26:24,170 --> 00:26:25,170
OK.

435
00:26:25,170 --> 00:26:34,580
And one more thing: you can teach up to three
concept at a time to the model.

436
00:26:34,580 --> 00:26:42,000
So the first concept is is: let's say it's
me, and in here I can also teach my wife picture

437
00:26:42,000 --> 00:26:43,000
for example.

438
00:26:43,000 --> 00:26:45,500
It can be like wife DB.

439
00:26:45,500 --> 00:26:52,559
So another folder and its classification data
set can be exactly same as the other one,

440
00:26:52,559 --> 00:26:55,909
or no, it wouldn't be, because it would be
related to women.

441
00:26:55,909 --> 00:26:59,440
Since it will be a woman, not man.

442
00:26:59,440 --> 00:27:06,610
Therefore, let's say woman images, and in
here you need to use another keyword for that.

443
00:27:06,610 --> 00:27:13,470
So it is important to find a rare keyword
from this list.

444
00:27:13,470 --> 00:27:21,990
I don't know which ones are very rare, but
a ske is commonly used at another prompt.

445
00:27:21,990 --> 00:27:26,800
So it can be like a ske woman.

446
00:27:26,800 --> 00:27:38,100
And in here it will be a photo of women and
sample will be a photo of a ske woman.

447
00:27:38,100 --> 00:27:40,660
OK, and the rest is same.

448
00:27:40,660 --> 00:27:43,159
And you can also add another concept here.

449
00:27:43,159 --> 00:27:50,789
But the only thing that matters is the class
of the another subject, If it.

450
00:27:50,789 --> 00:27:59,240
If it is a cat or a dog or a tree, whatever
you are teaching the class and instance prompt

451
00:27:59,240 --> 00:28:05,030
so that you can differently call them and
you can use both of them in a single picture.

452
00:28:05,030 --> 00:28:09,620
For example, you can generate pictures of
your wife and yourself in the same picture,

453
00:28:09,620 --> 00:28:12,610
or your dog and yourself in same picture.

454
00:28:12,610 --> 00:28:18,330
But for this tutorial I am not going to teach
multiple concepts, so it is up to you to teach

455
00:28:18,330 --> 00:28:19,330
or not.

456
00:28:19,330 --> 00:28:23,549
I will just teach a single concept.

457
00:28:23,549 --> 00:28:24,549
All right.

458
00:28:24,549 --> 00:28:27,860
Now we are moving to saving tab.

459
00:28:27,860 --> 00:28:33,150
In here you can enter a custom model name
for saving checkpoints and LoRA models.

460
00:28:33,150 --> 00:28:34,669
You can check out the half-model.

461
00:28:34,669 --> 00:28:41,480
They say that it doesn't decrease the quality,
but the checkpoints are smaller.

462
00:28:41,480 --> 00:28:46,330
I didn't test it so I can't say if it is 100
percent correct or not.

463
00:28:46,330 --> 00:28:51,760
So to keep the quality in max, I won't check
it.

464
00:28:51,760 --> 00:28:53,279
Save checkpoints to sub directory.

465
00:28:53,279 --> 00:28:55,210
You should make this.

466
00:28:55,210 --> 00:29:01,700
You should check this checkbox so that the
savings will be under Web UI ohwx.

467
00:29:01,700 --> 00:29:04,190
They won't get in the same directory.

468
00:29:04,190 --> 00:29:06,710
Now this is important to set.

469
00:29:06,710 --> 00:29:10,580
Generate a ckpt file when saving during training.

470
00:29:10,580 --> 00:29:17,220
If you don't check this, then let's say you
won't be able to test, load back and test

471
00:29:17,220 --> 00:29:22,659
the model at the 20 epoch or 40 epoch or 60
epoch.

472
00:29:22,659 --> 00:29:24,360
So you should check this out.

473
00:29:24,360 --> 00:29:27,830
You can also continue from that point using
that as a base mode.

474
00:29:27,830 --> 00:29:35,279
And you can also load that model and you can
do test inference on that.

475
00:29:35,279 --> 00:29:40,899
So this is important, but this will increase
your hard drive usage.

476
00:29:40,899 --> 00:29:41,899
Be careful with that.

477
00:29:41,899 --> 00:29:43,549
Generate a ckpt file when training completes.

478
00:29:43,549 --> 00:29:44,549
Yes.

479
00:29:44,549 --> 00:29:46,720
Generate a ckpt file when training is canceled.

480
00:29:46,720 --> 00:29:53,510
I'm not checking this because when I cancel
I don't want it to generate a ckpt.

481
00:29:53,510 --> 00:29:58,390
After canceling you can just load the model
and click ckpt and it will generate a ckpt

482
00:29:58,390 --> 00:30:00,799
file from the last saved weights.

483
00:30:00,799 --> 00:30:02,259
Now weights.

484
00:30:02,259 --> 00:30:08,230
You see there is also option to save separate
diffuser snapshots when saving during training.

485
00:30:08,230 --> 00:30:14,950
This option will generate weight files, like
you see here.

486
00:30:14,950 --> 00:30:21,980
So for demonstration purposes I will also
select this from later point you can just

487
00:30:21,980 --> 00:30:27,840
make them as a new model folder and then you
can continue your training from there.

488
00:30:27,840 --> 00:30:35,380
Alternatively, I believe you can generate
a new model from your saved ckpt file as a

489
00:30:35,380 --> 00:30:41,160
new source checkpoint and you can continue
from that saved checkpoint ckpt file.

490
00:30:41,160 --> 00:30:44,070
I think both should be same.

491
00:30:44,070 --> 00:30:45,760
OK.

492
00:30:45,760 --> 00:30:48,220
After you did settings, just click save settings.

493
00:30:48,220 --> 00:30:51,610
When you click train, I think it is automatically
also saving.

494
00:30:51,610 --> 00:30:56,130
Now I will generate the class images before
starting training.

495
00:30:56,130 --> 00:31:02,410
This will use the settings that I did set
in these options.

496
00:31:02,410 --> 00:31:05,720
And let's see what kind of class images we
are going to get.

497
00:31:05,720 --> 00:31:10,340
OK, so you see, it is generating 300 class
images for training.

498
00:31:10,340 --> 00:31:11,340
Why?

499
00:31:11,340 --> 00:31:18,909
Because currently I have no images in here,
but, as you can see, it is not working right

500
00:31:18,909 --> 00:31:19,909
now.

501
00:31:19,909 --> 00:31:20,980
So there is a mistake.

502
00:31:20,980 --> 00:31:25,549
Obviously, To solve this mistake, I will just
restart the application.

503
00:31:25,549 --> 00:31:28,190
OK, restart is completed.

504
00:31:28,190 --> 00:31:29,190
Let's refresh.

505
00:31:29,190 --> 00:31:32,020
Go back to our extensions tab.

506
00:31:32,020 --> 00:31:33,020
Check for updates.

507
00:31:33,020 --> 00:31:34,309
If there is an update.

508
00:31:34,309 --> 00:31:38,519
Yes, there is a new update during the video.

509
00:31:38,519 --> 00:31:41,389
The updates are coming, So let's just refresh.

510
00:31:41,389 --> 00:31:43,330
OK, refreshed.

511
00:31:43,330 --> 00:31:44,550
Let's go back to extensions.

512
00:31:44,550 --> 00:31:45,550
Check for updates.

513
00:31:45,550 --> 00:31:46,550
OK, we are at the last.

514
00:31:46,550 --> 00:31:52,490
Then let's go to DreamBooth, select our model
load settings.

515
00:31:52,490 --> 00:31:54,649
Go to the generate.

516
00:31:54,649 --> 00:31:59,490
Before generating, I will delete these incorrect
images first.

517
00:31:59,490 --> 00:32:00,490
Let me do that.

518
00:32:00,490 --> 00:32:07,169
Go to the pictures and in here, go to the
web UI tutorial.

519
00:32:07,169 --> 00:32:08,980
Ctrl-a shift-delete.

520
00:32:08,980 --> 00:32:11,040
Yes, all deleted.

521
00:32:11,040 --> 00:32:13,760
And just click generate class images.

522
00:32:13,760 --> 00:32:16,170
OK, let's see if any error again.

523
00:32:16,170 --> 00:32:20,970
OK, OK, I think error continues.

524
00:32:20,970 --> 00:32:27,040
So instead of these methods, I will use txt2image
tab to generate images.

525
00:32:27,040 --> 00:32:33,100
The only difference between these and using
text to image is: let me show you.

526
00:32:33,100 --> 00:32:37,230
Meanwhile, just let's restart the application.

527
00:32:37,230 --> 00:32:44,760
When you use, generate images like this will
also generate a text file, same name as the

528
00:32:44,760 --> 00:32:51,529
image name, and inside it it will write photo
of man as a description.

529
00:32:51,529 --> 00:33:00,010
So this is useful when you do [filewords]
training or when you do LoRA training, But

530
00:33:00,010 --> 00:33:04,929
for now it is not necessary for us.

531
00:33:04,929 --> 00:33:11,700
I just reported this bug also to the developer,
so I believe it will get fixed really quickly.

532
00:33:11,700 --> 00:33:19,110
OK, so we are going to generate our class
images from here.

533
00:33:19,110 --> 00:33:21,169
Classification images: photo of man.

534
00:33:21,169 --> 00:33:26,270
I'm just typing that setting the sampling
steps counts 40, setting CFG: 7.5.

535
00:33:26,270 --> 00:33:35,260
So this batch size means that processing multiple
images at the same epoch.

536
00:33:35,260 --> 00:33:39,010
It will use more GPU RAM, but it will make
it faster.

537
00:33:39,010 --> 00:33:40,330
And how many I need?

538
00:33:40,330 --> 00:33:41,850
I need 300.

539
00:33:41,850 --> 00:33:50,640
Therefore, I am going to set this as 38, like
this, and then just click generate.

540
00:33:50,640 --> 00:33:52,880
So now it will generate images.

541
00:33:52,880 --> 00:34:00,340
But make sure that the selected model here
you see, is same as the model that you used

542
00:34:00,340 --> 00:34:02,659
to generate your model.

543
00:34:02,659 --> 00:34:08,290
So in here, when you select your model for
training, it shows the base model source checkpoint.

544
00:34:08,290 --> 00:34:14,820
You see Stable Diffusion 1.5 pruned, and currently
I am generating same images from this model.

545
00:34:14,820 --> 00:34:20,010
So the generated images will be saved in text
to image folder.

546
00:34:20,010 --> 00:34:22,340
Let's open it by clicking here.

547
00:34:22,340 --> 00:34:27,879
OK, when I have clicked open folder in here.

548
00:34:27,879 --> 00:34:34,139
It didn't open because it says in the CMD
window: text to image images does not exist.

549
00:34:34,139 --> 00:34:35,139
After you create an image.

550
00:34:35,139 --> 00:34:40,050
It will be generated because, as I said, this
is a fresh installation to demonstrate you.

551
00:34:40,050 --> 00:34:42,810
Therefore, all of my settings here are also
default.

552
00:34:42,810 --> 00:34:46,070
I didn't change any of them.

553
00:34:46,070 --> 00:34:48,329
And there is one another thing that I want
to mention.

554
00:34:48,329 --> 00:34:56,609
In the DreamBooth model selection You will
see in the SD 1.x versions they has they have

555
00:34:56,609 --> 00:34:57,859
EMA or not.

556
00:34:57,859 --> 00:35:05,300
So if they have EMA, it will increase your
further training, fine tuning the model.

557
00:35:05,300 --> 00:35:09,130
So you should pick EMA version having models.

558
00:35:09,130 --> 00:35:11,580
It only exists in the 1.x versions.

559
00:35:11,580 --> 00:35:14,260
I think in the SD 2.0.

560
00:35:14,260 --> 00:35:20,329
In the 2.1 there is no model released with
has EMA features.

561
00:35:20,329 --> 00:35:22,990
OK, the first batch has been completed.

562
00:35:22,990 --> 00:35:23,990
Let's open the folder.

563
00:35:23,990 --> 00:35:26,400
Now the folder is opened.

564
00:35:26,400 --> 00:35:28,859
So these are photo of man.

565
00:35:28,859 --> 00:35:33,869
You see, there will be very weird images,
bad quality images, but they don't matter

566
00:35:33,869 --> 00:35:34,869
much.

567
00:35:34,869 --> 00:35:40,310
They are not very important as long as they
are generated by our checkpoint model.

568
00:35:40,310 --> 00:35:48,240
OK, after all of the images have been generated,
just select them all with control-C, then

569
00:35:48,240 --> 00:35:54,740
go back to your folder where you want to get
them saved web UI tutorial.

570
00:35:54,740 --> 00:35:58,880
I am just going to copy paste them in the
folder.

571
00:35:58,880 --> 00:36:04,170
OK, let's return back to our DreamBooth and
load settings.

572
00:36:04,170 --> 00:36:09,079
So now we have the sufficient amount of classification
images.

573
00:36:09,079 --> 00:36:12,210
Now we are ready to click start training.

574
00:36:12,210 --> 00:36:19,900
OK, when we start training, it will first
start by caching them out.

575
00:36:19,900 --> 00:36:23,700
We will see that.

576
00:36:23,700 --> 00:36:28,500
So you see, it says that it has found 300
regularization images.

577
00:36:28,500 --> 00:36:32,440
Therefore, it is not going to generate any
more images.

578
00:36:32,440 --> 00:36:35,849
Currently it is caching them.

579
00:36:35,849 --> 00:36:42,340
OK, after the caching has been completed,
you will see the training has been started.

580
00:36:42,340 --> 00:36:45,800
It is progressing step by step.

581
00:36:45,800 --> 00:36:48,460
You see 13, 14.

582
00:36:48,460 --> 00:36:55,069
If you get out of memory error, then you need
to try further decreasing memory usage.

583
00:36:55,069 --> 00:37:01,839
All of the low memory settings and high memory
settings are stated in the wiki.

584
00:37:01,839 --> 00:37:03,640
I will put this into the description.

585
00:37:03,640 --> 00:37:05,420
Also, you are seeing right now.

586
00:37:05,420 --> 00:37:08,200
High batch size, set gradients.

587
00:37:08,200 --> 00:37:13,170
These will increase your memory usage and
these will decrease your memory usage.

588
00:37:13,170 --> 00:37:18,400
There is not much else things that you can
do, and one another thing is that the developers

589
00:37:18,400 --> 00:37:26,089
are constantly trying to optimize and improve
the extension to reduce memory usage.

590
00:37:26,089 --> 00:37:33,860
So therefore, when you watch this video, or
maybe one month later, you, your card, could

591
00:37:33,860 --> 00:37:37,940
perhaps do use DreamBooth training.

592
00:37:37,940 --> 00:37:41,140
So that's another possibility.

593
00:37:41,140 --> 00:37:45,599
And after how many steps we are going to see
our first sample images?

594
00:37:45,599 --> 00:37:47,130
We can calculate it easily.

595
00:37:47,130 --> 00:37:49,480
In the settings tab.

596
00:37:49,480 --> 00:37:53,240
We did set as 10 epoch and how many training
images we have.

597
00:37:53,240 --> 00:37:56,000
We have 12, you see in here.

598
00:37:56,000 --> 00:38:05,119
Therefore, after 120 steps we are going to
see our first sample training sample images.

599
00:38:05,119 --> 00:38:12,240
Actually, on, after 120 steps, it will save
the checkpoint.

600
00:38:12,240 --> 00:38:15,339
After 60 steps, because we did set 5 epochs.

601
00:38:15,339 --> 00:38:20,590
We are going to see the first sample image
and 60 steps has been completed.

602
00:38:20,590 --> 00:38:24,040
So it is generating preview images at the
step 60.

603
00:38:24,040 --> 00:38:28,130
Ok, the first samples have been generated.

604
00:38:28,130 --> 00:38:29,690
Let's open the samples folder.

605
00:38:29,690 --> 00:38:34,190
So where they were saved, they were saved
under our model.

606
00:38:34,190 --> 00:38:41,250
Let me show: Ok, I have so many same tabs.

607
00:38:41,250 --> 00:38:47,020
Ok, inside our installation folder, go to
the models and in here go to the DreamBooth

608
00:38:47,020 --> 00:38:50,890
and in here you see the same name as our training
model name.

609
00:38:50,890 --> 00:38:52,200
Enter there.

610
00:38:52,200 --> 00:38:53,880
In here you will see samples.

611
00:38:53,880 --> 00:38:56,700
When you click here you will see the samples.

612
00:38:56,700 --> 00:39:05,280
So the first sample is generated with this
sample prompt with ohwx man.

613
00:39:05,280 --> 00:39:09,810
So this is our class and this is the unique
instance prompt we have set.

614
00:39:09,810 --> 00:39:11,819
Ok, so there is another image.

615
00:39:11,819 --> 00:39:12,819
You see.

616
00:39:12,819 --> 00:39:19,100
This is generated with photo of ohwx man by
Tomer Hanuka.

617
00:39:19,100 --> 00:39:22,380
Why did I set this and where did I set this?

618
00:39:22,380 --> 00:39:24,400
I did set this in here.

619
00:39:24,400 --> 00:39:35,109
If you remember, The second prompt you see
in here with name it as one, is the sanity

620
00:39:35,109 --> 00:39:38,480
sample prompt.

621
00:39:38,480 --> 00:39:43,940
The number here is the step count that it
has been generated, and this is the other

622
00:39:43,940 --> 00:39:47,180
thing is the prompt used to generate it.

623
00:39:47,180 --> 00:39:54,660
After we progress in the training, you will
understand why we are using this.

624
00:39:54,660 --> 00:40:00,600
As much as this image looks like us, with
a different style, it means that our model

625
00:40:00,600 --> 00:40:07,210
is learning good, and when it becomes exactly
like us, not styled like this, that would

626
00:40:07,210 --> 00:40:12,089
mean that our model is overtrained and now
we can't apply styles.

627
00:40:12,089 --> 00:40:22,640
Our aim is learning our teaching our shape,
but not overtraining it, not distributing,

628
00:40:22,640 --> 00:40:29,040
disturbing the underlying context, the knowledge
of it, not overriding it completely.

629
00:40:29,040 --> 00:40:32,690
So after we progress in the training, we will
understand better.

630
00:40:32,690 --> 00:40:40,590
Okay, now let me explain to you to how to
prepare your training dataset images.

631
00:40:40,590 --> 00:40:45,829
What is important with the selection of the
images?

632
00:40:45,829 --> 00:40:50,569
What we want to teach is the subject that
we want to teach.

633
00:40:50,569 --> 00:40:52,170
The most important part.

634
00:40:52,170 --> 00:40:54,250
I want to teach my face.

635
00:40:54,250 --> 00:40:59,070
Therefore, other than my face, everything
must be different, or, let's say, should be

636
00:40:59,070 --> 00:41:01,080
different in each of the images.

637
00:41:01,080 --> 00:41:03,890
So, other than face, what can be different?

638
00:41:03,890 --> 00:41:07,830
My clothes and the background can be different.

639
00:41:07,830 --> 00:41:14,240
So if you are teaching your face other than
your face, all of the backgrounds and the

640
00:41:14,240 --> 00:41:17,540
clothes should be different as much as possible.

641
00:41:17,540 --> 00:41:23,500
As you can see in my pictures, I have made
sure that all of the backgrounds and the clothes

642
00:41:23,500 --> 00:41:27,740
are different or the clothes are not visible.

643
00:41:27,740 --> 00:41:34,030
So if you make your clothes different and
your backgrounds are different, then the model

644
00:41:34,030 --> 00:41:37,640
will learn your face, not your clothes or
not the backgrounds.

645
00:41:37,640 --> 00:41:38,650
That is what we want.

646
00:41:38,650 --> 00:41:42,460
We want to teach our face, not the other things
in the pictures.

647
00:41:42,460 --> 00:41:49,650
If you use same clothes, then the model will
not say that this is the face and this is

648
00:41:49,650 --> 00:41:53,530
the clothes and the model will learn both
of them at the same time and it will reduce

649
00:41:53,530 --> 00:41:57,060
your stylizing your face.

650
00:41:57,060 --> 00:42:04,560
Therefore, the key point of preparing training
images is having different things other than

651
00:42:04,560 --> 00:42:05,560
the subject.

652
00:42:05,560 --> 00:42:09,069
So if the subject is face, the other things
must be different.

653
00:42:09,069 --> 00:42:18,010
Also, you should have different angles of
photos and different distances of photos.

654
00:42:18,010 --> 00:42:25,540
It will make the model learn different angles
and different distances to generate different

655
00:42:25,540 --> 00:42:30,339
kinds of different styles, more variety of
images.

656
00:42:30,339 --> 00:42:37,119
So if you make your images, I can't say my
data set is the best available data set.

657
00:42:37,119 --> 00:42:42,730
You can expand your data set with more variety
of images, more variety of poses, more variety

658
00:42:42,730 --> 00:42:46,720
of angles, more variety of lightning.

659
00:42:46,720 --> 00:42:47,760
Lightning also matters.

660
00:42:47,760 --> 00:42:49,280
It would be better.

661
00:42:49,280 --> 00:42:55,200
However, this is a small data set and I think
it is working pretty decently.

662
00:42:55,200 --> 00:43:00,520
But if you expand this data set, your training
data set, with more variety, then it is better.

663
00:43:00,520 --> 00:43:08,000
It will learn your face or subject in a more
generalized matter and with that way we will

664
00:43:08,000 --> 00:43:14,410
be able to produce different kind of different
artistic images more easily.

665
00:43:14,410 --> 00:43:21,880
Okay, so you see, currently it is compiling
a checkpoint ckpt file and you can just load

666
00:43:21,880 --> 00:43:27,090
the ckpt file directly and do inference on
that checkpoint.

667
00:43:27,090 --> 00:43:34,770
It is compiling checkpoint at the step 360,
which is epoch 30, and so where are these

668
00:43:34,770 --> 00:43:37,130
checkpoint files are located?

669
00:43:37,130 --> 00:43:45,320
They are located on models inside inside our
folder, and you see the ckpt file and the

670
00:43:45,320 --> 00:43:48,300
yaml file is here.

671
00:43:48,300 --> 00:43:55,609
If you don't know what are yaml files, just
watch my how to use Stable Diffusion 2.1 and

672
00:43:55,609 --> 00:43:59,359
different models in the web ui tutorial video.

673
00:43:59,359 --> 00:44:07,940
I will put the link as usual, and let's check
out our so far samples.

674
00:44:07,940 --> 00:44:13,170
So in this image this is like me, but no other
sample prompts are like us.

675
00:44:13,170 --> 00:44:15,260
We just need to do more training.

676
00:44:15,260 --> 00:44:22,250
And also in this screen you will see 5.5 or
3.7.

677
00:44:22,250 --> 00:44:30,540
So this means that this is how many iterations
per iteration is done in each second.

678
00:44:30,540 --> 00:44:37,040
However, these values are not very correctly
displayed, so there is also loss and this

679
00:44:37,040 --> 00:44:38,400
lr is important.

680
00:44:38,400 --> 00:44:40,120
This shows your learning rate.

681
00:44:40,120 --> 00:44:43,850
So 2e-6, what does that mean?

682
00:44:43,850 --> 00:44:47,599
That means that it is a number.

683
00:44:47,599 --> 00:44:54,119
When you type it to the google 2e-6 and go
to the first result, for example, it will

684
00:44:54,119 --> 00:44:57,339
show you it is equal to this number.

685
00:44:57,339 --> 00:44:58,980
Okay, so this is the number.

686
00:44:58,980 --> 00:45:04,740
Actually we did set in our settings, in our
learning rate, you see.

687
00:45:04,740 --> 00:45:10,190
So this is equivalent of the scientific e-notation
number.

688
00:45:10,190 --> 00:45:17,280
If you set changing numbers from here you
see there are changing numbers like polynomial,

689
00:45:17,280 --> 00:45:25,560
constant or other things, learning rates then
you will see different numbers in here and

690
00:45:25,560 --> 00:45:27,359
it also shows the gpu usage.

691
00:45:27,359 --> 00:45:30,720
However, this is also not very accurate.

692
00:45:30,720 --> 00:45:34,710
It says that 9.5 gigabytes currently is being
used.

693
00:45:34,710 --> 00:45:41,160
Okay, okay, it has been 72, 82 epochs.

694
00:45:41,160 --> 00:45:46,730
Now i will show you how you can continue training,
if an error occurs.

695
00:45:46,730 --> 00:45:51,440
So to illustrate that, i will just crash the
application with closing here.

696
00:45:51,440 --> 00:45:55,890
When you close from here, it won't save any
checkpoint or anything.

697
00:45:55,890 --> 00:45:58,360
Use the error connection error.

698
00:45:58,360 --> 00:46:08,290
Then just restart the application and after
the restart is done, just refresh your interface.

699
00:46:08,290 --> 00:46:14,440
Go to the DreamBooth tab, select the model,
click load settings it actually it will be

700
00:46:14,440 --> 00:46:15,440
automatically loaded.

701
00:46:15,440 --> 00:46:17,500
And then just click train.

702
00:46:17,500 --> 00:46:23,359
It will continue from the last checkpoint,
which is 80 epochs.

703
00:46:23,359 --> 00:46:25,140
Let's wait.

704
00:46:25,140 --> 00:46:28,609
Okay, you see it has.

705
00:46:28,609 --> 00:46:34,579
It is continuing from wherever it is left,
as you can see here.

706
00:46:34,579 --> 00:46:42,630
Also, in the cmd window it shows first resume
epoch, and first resume step, step, as you,

707
00:46:42,630 --> 00:46:43,740
as you can see here.

708
00:46:43,740 --> 00:46:52,480
Okay, we are over 168 epochs and we are already
doing a lot of over training.

709
00:46:52,480 --> 00:46:54,910
How do i know?

710
00:46:54,910 --> 00:47:03,619
As i said you in the beginning, i have entered
a sanity, sanity prompt.

711
00:47:03,619 --> 00:47:09,950
So the samples numbered with, dash one are
the sanity prompts.

712
00:47:09,950 --> 00:47:12,830
And let's look at the sanity prompts changes.

713
00:47:12,830 --> 00:47:15,580
So the sanity prompts started like this.

714
00:47:15,580 --> 00:47:23,310
Then in here you see, the sanity prompt is
resembling me and also here resembling me,

715
00:47:23,310 --> 00:47:26,460
okay, resembling me somehow.

716
00:47:26,460 --> 00:47:37,660
And after certain point, actually after 1368
steps, the sanity prompts become just like

717
00:47:37,660 --> 00:47:38,660
me.

718
00:47:38,660 --> 00:47:45,410
You see, it is not anymore styled okay, like
this, like this and this is almost as like

719
00:47:45,410 --> 00:47:50,089
me, and you see, they are not anymore styled
like here.

720
00:47:50,089 --> 00:47:53,690
Styling is completely gone and in here.

721
00:47:53,690 --> 00:47:59,650
Therefore, now we are sure that we are doing
over training.

722
00:47:59,650 --> 00:48:08,550
So i am just going to stop training with cancel
and i am going to use different checkpoints,

723
00:48:08,550 --> 00:48:11,710
test them out to see how they are performing.

724
00:48:11,710 --> 00:48:17,320
Now the hard part is coming: the prompting,
the proper, the correct prompting to obtain

725
00:48:17,320 --> 00:48:19,940
the good results.

726
00:48:19,940 --> 00:48:22,870
So the training has been cancelled.

727
00:48:22,870 --> 00:48:25,540
Let's look for the closest one.

728
00:48:25,540 --> 00:48:32,440
I am refreshing here and in here, yes, this
one looks like the closest one: 1308.

729
00:48:32,440 --> 00:48:35,359
Then go to the text2image tab.

730
00:48:35,359 --> 00:48:39,000
So how are we going to generate our own image?

731
00:48:39,000 --> 00:48:41,099
We are going to use photo of.

732
00:48:41,099 --> 00:48:47,290
These two keywords are also associated with
us right now, but not as strong as our prompt

733
00:48:47,290 --> 00:48:49,770
instance.

734
00:48:49,770 --> 00:48:50,770
Ohwx and man.

735
00:48:50,770 --> 00:48:57,690
Also man is very much associated, okay, so
when we type like this and hit the generate

736
00:48:57,690 --> 00:49:00,099
button, it will generate our own image.

737
00:49:00,099 --> 00:49:02,140
Okay, the image is ready.

738
00:49:02,140 --> 00:49:06,750
You see, it is like us and now we need to
style it.

739
00:49:06,750 --> 00:49:12,950
So let's add in this name style and let's
see what kind of result we are going to get.

740
00:49:12,950 --> 00:49:21,390
Okay, as you can see, we didn't get much of
styling, so therefore, i am going to show

741
00:49:21,390 --> 00:49:26,380
you an extension which is named as web ui
prompt generator.

742
00:49:26,380 --> 00:49:29,130
You can install it from available tab.

743
00:49:29,130 --> 00:49:34,150
Just click load and in here just search for
prompt and you will see prompt generator and

744
00:49:34,150 --> 00:49:38,770
just click install and then just apply and
restart the ui.

745
00:49:38,770 --> 00:49:41,340
After that you will see prompt generator tab
here.

746
00:49:41,340 --> 00:49:47,839
So let's get some extra additional keywords
from prompt generator and let's click generate.

747
00:49:47,839 --> 00:49:55,670
Okay, there are a lot of results here, but,
this came to me, could work like, so i copied

748
00:49:55,670 --> 00:50:00,630
it and pasted it in here and let's see the
result we are going to get.

749
00:50:00,630 --> 00:50:06,210
Okay, we got somewhat decent results, but
it is still not very much like us.

750
00:50:06,210 --> 00:50:10,920
Therefore, we need to increase the prompt
strength.

751
00:50:10,920 --> 00:50:12,500
So what is prompt strength?

752
00:50:12,500 --> 00:50:13,500
prompt attention.

753
00:50:13,500 --> 00:50:17,380
This is from the official wiki of the Automatic1111.

754
00:50:17,380 --> 00:50:24,750
So if you want to increase attention to a
word by factor of 1.1, you can take the word

755
00:50:24,750 --> 00:50:26,380
inside one parentheses.

756
00:50:26,380 --> 00:50:34,930
If you want to increase the attention even
more by factor of 1.2, 21, so you can just

757
00:50:34,930 --> 00:50:40,520
put like this: alternatively, you can use
an easier way, which will be: let me show

758
00:50:40,520 --> 00:50:47,010
me, let me also zoom in, just type like this:
okay, so this will increase the attention.

759
00:50:47,010 --> 00:50:56,470
This will force model to generate image that
is more like us and it will going to ignore

760
00:50:56,470 --> 00:50:57,650
the rest.

761
00:50:57,650 --> 00:51:05,130
Also, in this prompt there are so many things
that would be unrelated to disney style.

762
00:51:05,130 --> 00:51:13,960
So what would be related to disney style,
for example, CGI, and let's also add some

763
00:51:13,960 --> 00:51:15,049
other keywords.

764
00:51:15,049 --> 00:51:17,270
Okay, here are results.

765
00:51:17,270 --> 00:51:21,060
Not very much like us and not very good quality.

766
00:51:21,060 --> 00:51:28,120
We need to improve the prompt with adding
some negative prompts as well.

767
00:51:28,120 --> 00:51:35,049
Okay, here i have added some negative prompts
and now you see we have a much better artwork,

768
00:51:35,049 --> 00:51:39,349
but still not very much resembling to me.

769
00:51:39,349 --> 00:51:47,640
So i am going to try another prompt with also
increasing, the, the emphasis of our unique

770
00:51:47,640 --> 00:51:51,290
keyword, which is ohwx and the man.

771
00:51:51,290 --> 00:51:58,130
In every prompt you must have ohwx man with
some increased strength, probably to get your

772
00:51:58,130 --> 00:52:01,340
own face, and also adding photo of.

773
00:52:01,340 --> 00:52:02,340
Why?

774
00:52:02,340 --> 00:52:09,059
Because during the training we have used class
prompt as photo of man.

775
00:52:09,059 --> 00:52:15,849
Therefore, now these three keywords are also
associated with us, but the most association

776
00:52:15,849 --> 00:52:18,890
is coming from ohwx, okay.

777
00:52:18,890 --> 00:52:27,010
Okay, so i am going to try with emphasis of
1.5 and a new prompt like this.

778
00:52:27,010 --> 00:52:28,260
Let's see the results.

779
00:52:28,260 --> 00:52:32,859
Okay, we got an image that is not very stylized.

780
00:52:32,859 --> 00:52:35,370
Therefore, we need to increase CFG.

781
00:52:35,370 --> 00:52:36,370
So what is CFG?

782
00:52:36,370 --> 00:52:43,020
CFG is classifier free guidance scale how
strongly the image should conform the prompt.

783
00:52:43,020 --> 00:52:45,130
Lower values produce more creative results.

784
00:52:45,130 --> 00:52:54,970
We want the model to obey our prompt because
we are providing a very detailed prompt.

785
00:52:54,970 --> 00:52:59,579
Therefore, we need to increase scale and try
it.

786
00:52:59,579 --> 00:53:04,530
So i will show you how you can try multiple
scale values.

787
00:53:04,530 --> 00:53:09,690
Go to the bottom on here and go to the x/y
plot.

788
00:53:09,690 --> 00:53:14,190
So in the x/y plot there are x and y values.

789
00:53:14,190 --> 00:53:15,720
Currently we only need x value.

790
00:53:15,720 --> 00:53:22,150
In the x value i am going to select CFG scale
and in here i am just typing seven, eight,

791
00:53:22,150 --> 00:53:29,700
nine, ten, eleven, twelve, thirteen, fourteen,
fifteen, sixteen, and i wanted to use same

792
00:53:29,700 --> 00:53:37,869
seed for all of the input so that i can see
the changes, and i will generate four images

793
00:53:37,869 --> 00:53:42,240
in each iteration, in each step.

794
00:53:42,240 --> 00:53:45,319
My graphic card is able to process four images.

795
00:53:45,319 --> 00:53:48,609
If you don't have much vram, you can't do
that.

796
00:53:48,609 --> 00:53:50,990
Then you should increase this.

797
00:53:50,990 --> 00:53:54,510
Okay, if you check this, keep minus one for
seeds.

798
00:53:54,510 --> 00:53:58,770
Then each image in the each generation would
be different.

799
00:53:58,770 --> 00:54:04,520
However, i want to see the difference of CFG
effect in a legend.

800
00:54:04,520 --> 00:54:09,650
Therefore, i'm keeping it like this and then
just click generate.

801
00:54:09,650 --> 00:54:19,260
So currently, in the CMD window, actually
it is generating four images at each epoch.

802
00:54:19,260 --> 00:54:23,380
So you see, in the 20 steps, actually it is
processing 80 steps.

803
00:54:23,380 --> 00:54:32,480
So four of them is being parallelly processed,
since i did set batch size to four.

804
00:54:32,480 --> 00:54:33,590
Okay, CFG images.

805
00:54:33,590 --> 00:54:35,760
Different CFG images have been generated.

806
00:54:35,760 --> 00:54:40,910
I have modified the input because the previous
input was not very good.

807
00:54:40,910 --> 00:54:42,930
Actually, it turns out that.

808
00:54:42,930 --> 00:54:47,869
But it is not important, because when you
are working with Stable Diffusion, you have

809
00:54:47,869 --> 00:54:55,340
to make, you have to generate a lot of images
to find out the good ones that you, you would

810
00:54:55,340 --> 00:54:57,619
like to obtain.

811
00:54:57,619 --> 00:55:01,990
So let's look at, look at the effect of the
CFG.

812
00:55:01,990 --> 00:55:04,920
So this is our seed value.

813
00:55:04,920 --> 00:55:11,960
If you use this seed value, you will always
generate similar images in each generation,

814
00:55:11,960 --> 00:55:16,359
as long as you keep the same value, same model.

815
00:55:16,359 --> 00:55:18,369
So this is the CFG scale seven.

816
00:55:18,369 --> 00:55:23,710
At the CFG scale seven, there is not much
resemblance.

817
00:55:23,710 --> 00:55:28,220
At the CFG scale eight, a little bit resemblance.

818
00:55:28,220 --> 00:55:31,220
Look at how the images are changing.

819
00:55:31,220 --> 00:55:33,819
This is CFG scale nine.

820
00:55:33,819 --> 00:55:36,390
There is some resemblance in these two.

821
00:55:36,390 --> 00:55:39,480
Okay, and if in the CFG scale 10.

822
00:55:39,480 --> 00:55:47,119
Now, this is also some resembles and in here,
okay, you see, resembles is increasing and

823
00:55:47,119 --> 00:55:53,510
in the CFG scale 14 actually, there is really
good resemblance in this image and in this

824
00:55:53,510 --> 00:56:00,280
image actually, and so it goes, and after
certain CFG scale it becomes, i think the

825
00:56:00,280 --> 00:56:03,400
quality starts to be decreasing.

826
00:56:03,400 --> 00:56:06,940
So therefore the CFG scale makes difference.

827
00:56:06,940 --> 00:56:14,589
Now let's say you want to test out different
artists, styles with different CFG scales.

828
00:56:14,589 --> 00:56:17,140
How can you do that?

829
00:56:17,140 --> 00:56:23,670
I am putting here a special keyword that i
am going to use by replace kw.

830
00:56:23,670 --> 00:56:30,299
Okay, then the rest is anything you want,
and in the bottom so this time i am going

831
00:56:30,299 --> 00:56:33,540
to select prompt sr.

832
00:56:33,540 --> 00:56:39,180
Okay, so the prompt sr works as separate a
list of words with commas, and the first word

833
00:56:39,180 --> 00:56:40,590
will be used as a keyword.

834
00:56:40,590 --> 00:56:45,359
Script will search for this word in the prompt
and replace it with others.

835
00:56:45,359 --> 00:56:49,069
So these keywords will be replaced whatever
i type here.

836
00:56:49,069 --> 00:56:57,980
So let's say wlob and then artgerm and then
whatever other artists that you want to test.

837
00:56:57,980 --> 00:57:02,630
Okay, i have added two more artists, so we
have four artists.

838
00:57:02,630 --> 00:57:10,400
Let's also test 4 CFG values 10, 11, 12 and
13.

839
00:57:10,400 --> 00:57:23,869
Perhaps let's start from 11, okay, and let's
keep seeds for minus one, but that time we

840
00:57:23,869 --> 00:57:27,390
we couldn't test the CFG or the style.

841
00:57:27,390 --> 00:57:34,460
Therefore, let's keep the same seed, okay,
and you see, there are restore faces, tiling

842
00:57:34,460 --> 00:57:40,270
and high res fix, so you could also pick them
to improve your output, but that would take

843
00:57:40,270 --> 00:57:45,930
extra time and you can do them in the extras
tab which i will show.

844
00:57:45,930 --> 00:57:48,309
And the batch count is one and batch size
is four.

845
00:57:48,309 --> 00:57:50,990
Let's see what kind of results we are going
to get.

846
00:57:50,990 --> 00:57:56,750
By the way, these other keywords will also
heavily affect the artist style.

847
00:57:56,750 --> 00:58:03,220
Therefore, if you want to only check out the
artist style, then you should reduce number

848
00:58:03,220 --> 00:58:08,750
of extra keywords here and let's see what
we are going to get.

849
00:58:08,750 --> 00:58:11,580
Okay, i did get runtime error.

850
00:58:11,580 --> 00:58:12,580
Why?

851
00:58:12,580 --> 00:58:16,190
Because i have forgotten to put this keyword
in here.

852
00:58:16,190 --> 00:58:18,330
The first keyword has to be that.

853
00:58:18,330 --> 00:58:20,880
Now i need to run again.

854
00:58:20,880 --> 00:58:23,819
Okay, now the generation started.

855
00:58:23,819 --> 00:58:28,910
You should always, check out the CMD window
and what is happening here.

856
00:58:28,910 --> 00:58:32,300
If you get an error, then you should fix it,
obviously.

857
00:58:32,300 --> 00:58:35,930
Okay, this is the kind of tile that we are
going to get.

858
00:58:35,930 --> 00:58:37,710
Actually, it is pretty useful.

859
00:58:37,710 --> 00:58:45,551
So, you see, in the top CFG scale and in the
left we got the art style, by the way: It

860
00:58:45,551 --> 00:58:56,010
also produces results with: replacekw and
not much like representing the style or me.

861
00:58:56,010 --> 00:59:07,650
Therefore, perhaps we can remove many of the
keywords that would take away the style like:

862
00:59:07,650 --> 00:59:10,180
let me do.

863
00:59:10,180 --> 00:59:17,349
Okay this time we have more kind of styling,
as you can see here: this is the default,

864
00:59:17,349 --> 00:59:21,859
this is wlobe, this is artgerm.

865
00:59:21,859 --> 00:59:25,290
This is Robert S Duncanson and this is Karol
Bak.

866
00:59:25,290 --> 00:59:31,950
Especially Karol Bak style is pretty different
and significant, as you can see.

867
00:59:31,950 --> 00:59:38,030
So the key point here is is, with Stable Diffusion,
that you have to generate a lot of images,

868
00:59:38,030 --> 00:59:44,250
and some of them will be very, very good and
maybe majority of them will not be good and

869
00:59:44,250 --> 00:59:45,250
useful.

870
00:59:45,250 --> 00:59:55,470
This is the nature of the AI based art generation,
especially if you are trying to generate art

871
00:59:55,470 --> 01:00:04,119
based on your subject, a new subject, and
also when we were doing training in here,

872
01:00:04,119 --> 01:00:08,109
you can use more classification images.

873
01:00:08,109 --> 01:00:09,990
That can help.

874
01:00:09,990 --> 01:00:18,730
I said that the community is using 300 total,
but that is not a hard limit.

875
01:00:18,730 --> 01:00:25,579
You can just use 200 images for per training
image and that may help you to improve your

876
01:00:25,579 --> 01:00:26,579
style.

877
01:00:26,579 --> 01:00:30,700
Actually, it is also the number used in the
official paper, as i said.

878
01:00:30,700 --> 01:00:31,700
So it is up to you.

879
01:00:31,700 --> 01:00:33,620
You have to do experimenting.

880
01:00:33,620 --> 01:00:39,369
The numbers and the quality you get also totally
will depend on your training data set.

881
01:00:39,369 --> 01:00:47,210
If you get a much variety having a training
data set, as i have explained, then your model

882
01:00:47,210 --> 01:00:49,030
can learn much better.

883
01:00:49,030 --> 01:00:50,910
I will show you one another thing here.

884
01:00:50,910 --> 01:00:57,049
There is a prompt matrix that will generate
combination of the images.

885
01:00:57,049 --> 01:01:05,280
Okay, so when you type your query like this
and select it from matrix, this query will

886
01:01:05,280 --> 01:01:10,260
become face photo of ohwx man:1.3, like this.

887
01:01:10,260 --> 01:01:15,370
And then they will get combined by like this.

888
01:01:15,370 --> 01:01:24,030
So this will be generate all of the combinations
of the written text separated with the let

889
01:01:24,030 --> 01:01:28,579
me tell you once again, vertical pipe character.

890
01:01:28,579 --> 01:01:32,530
It will generate all of these keywords combination
like this.

891
01:01:32,530 --> 01:01:34,580
Okay, i will show one another thing.

892
01:01:34,580 --> 01:01:40,450
Let's say you are going to sleep and you want
your computer to generate many different style

893
01:01:40,450 --> 01:01:44,240
of images for you during your sleep.

894
01:01:44,240 --> 01:01:49,289
For that i will show you an easy way to do
it.

895
01:01:49,289 --> 01:01:58,890
So our first our first prompt is face photo
of ohwx and let's say 1.4.

896
01:01:58,890 --> 01:02:06,059
Then let's add some certain keywords to get
some certain kind of prompt.

897
01:02:06,059 --> 01:02:14,010
Okay, i have typed like this and generated
20 input like this, then it has generated

898
01:02:14,010 --> 01:02:15,609
me a lot of results.

899
01:02:15,609 --> 01:02:23,940
I am going to copy all of this into a notepad
file, paste it so you see they are actually

900
01:02:23,940 --> 01:02:26,100
copied as one line each one.

901
01:02:26,100 --> 01:02:28,450
Then i will generate several more.

902
01:02:28,450 --> 01:02:35,240
Okay, i keep copy, pasting the newly generated
input to there.

903
01:02:35,240 --> 01:02:40,360
Okay, now i have 60 lines of inputs like this.

904
01:02:40,360 --> 01:02:45,940
I am going to save it as.

905
01:02:45,940 --> 01:02:49,380
Let's go to the pictures and nightly prompts.

906
01:02:49,380 --> 01:03:00,270
Okay, then go back to text2img tab and in
here select prompts from file or text box.

907
01:03:00,270 --> 01:03:05,640
You can paste all of them here or you can
upload them from here.

908
01:03:05,640 --> 01:03:13,720
So i will upload them, from the text box,
from the text file, and they are all uploaded.

909
01:03:13,720 --> 01:03:19,920
I am going to say, use random seed for all
lines, because i want to get as many as possibly

910
01:03:19,920 --> 01:03:22,329
different results.

911
01:03:22,329 --> 01:03:29,339
And then i want to generate how many images
you want to generate for each one.

912
01:03:29,339 --> 01:03:36,339
I want to generate, let's say, eight images
in parallel, and currently they will use the

913
01:03:36,339 --> 01:03:39,490
CFG value I am going to set here 14.

914
01:03:39,490 --> 01:03:48,599
So with 60 and 8 images batch size, we are
going to get 480 images.

915
01:03:48,599 --> 01:03:53,260
Let's say you want to generate 4000 or whatever
you want.

916
01:03:53,260 --> 01:04:02,720
So if i set this 20, we are going to get exactly
20 times multiplied by 8 and multiplied by

917
01:04:02,720 --> 01:04:05,750
the number of lines we have.

918
01:04:05,750 --> 01:04:13,700
9600 images during the night with a lot of
different inputs, variation, and among them

919
01:04:13,700 --> 01:04:17,890
you can pick whatever you want and use it
as you want.

920
01:04:17,890 --> 01:04:20,609
This is one of the options that you can.

921
01:04:20,609 --> 01:04:25,279
Okay, after i click it, it started generating
images.

922
01:04:25,279 --> 01:04:31,910
For example, generated this one, and if you
wonder what is this image, you go to the png

923
01:04:31,910 --> 01:04:39,950
info and then you can just go to get the image,
drag and drop it in here and it will show

924
01:04:39,950 --> 01:04:42,500
you all of the parameters it has.

925
01:04:42,500 --> 01:04:47,549
So this is the prompt input and this is the
negative prompt input it has and the number

926
01:04:47,549 --> 01:04:48,980
of steps used.

927
01:04:48,980 --> 01:04:55,609
The sampler used, the CFG scale used, the
seed, so with this seed you can repeat this

928
01:04:55,609 --> 01:04:57,309
image generated.

929
01:04:57,309 --> 01:05:02,940
You can use this seed and change the CFG value
and generate other variations of this and

930
01:05:02,940 --> 01:05:04,750
the size and the model hash.

931
01:05:04,750 --> 01:05:10,390
The model hash, of course, will change since,
we are using our custom trained model.

932
01:05:10,390 --> 01:05:15,090
The batch size and the batch position, so
this is also important.

933
01:05:15,090 --> 01:05:23,039
To exactly get this, you need to generate
a against batch size as 8, and the sixth position

934
01:05:23,039 --> 01:05:24,039
will be this one.

935
01:05:24,039 --> 01:05:28,910
If you use this seed and this CFG value and
this sampler.

936
01:05:28,910 --> 01:05:34,130
We are getting some decent photos, and i will
leave it to run during my sleep and tomorrow

937
01:05:34,130 --> 01:05:38,700
i will show you, of course, in a moment for
you.

938
01:05:38,700 --> 01:05:43,080
We are going to see what kind of good images
we got.

939
01:05:43,080 --> 01:05:49,220
Okay, here you see, some of the images i have
generated during my sleep.

940
01:05:49,220 --> 01:05:52,320
They are pretty good quality, but they are
very similar.

941
01:05:52,320 --> 01:05:53,320
Why?

942
01:05:53,320 --> 01:05:59,829
Because it appears that the inputs i have
used to generate them were not much different.

943
01:05:59,829 --> 01:06:02,990
However, some of them are really high quality.

944
01:06:02,990 --> 01:06:07,390
For example, this image: you see, it has almost
perfect eyes, perfect shape.

945
01:06:07,390 --> 01:06:10,390
It's a really good quality image.

946
01:06:10,390 --> 01:06:16,220
So your training data set and the keywords,
the prompts you use, will hundred percent

947
01:06:16,220 --> 01:06:23,160
affect the outcome that you are going to get,
and you really need to stylize your prompt

948
01:06:23,160 --> 01:06:25,529
according to what you want to get.

949
01:06:25,529 --> 01:06:30,120
Now let me show you a few of the prompts used
for generating these images.

950
01:06:30,120 --> 01:06:36,490
To doing that, i am going png info, okay,
and then i will drag and drop.

951
01:06:36,490 --> 01:06:41,240
For example, let's first see a 3d like image.

952
01:06:41,240 --> 01:06:51,500
Okay, and you see this used blender, zbrush,
autodesk maya, unreal engine, colored, because

953
01:06:51,500 --> 01:06:57,829
if you want to generate a 3d like image then
you need to use these kind of keywords.

954
01:06:57,829 --> 01:06:59,160
Then you can send these to.

955
01:06:59,160 --> 01:07:02,260
For example, let's go to the extras tab.

956
01:07:02,260 --> 01:07:08,270
In extras tab i can upscale this image to
get it a bigger size.

957
01:07:08,270 --> 01:07:14,950
After my testing i have found that R-ESRGAN
4x+ works best.

958
01:07:14,950 --> 01:07:18,810
There is also anime version.

959
01:07:18,810 --> 01:07:25,850
Also, LDSR is working very good, but this
requires a lot of gpu memory.

960
01:07:25,850 --> 01:07:31,480
So when i click generate, when the first time
you generate it, it is going to download the

961
01:07:31,480 --> 01:07:35,740
model that is necessary for R-ESRGAN 4x+.

962
01:07:35,740 --> 01:07:41,220
You can see here and now we will see the upscaled
image.

963
01:07:41,220 --> 01:07:43,190
So this is the upscaled image.

964
01:07:43,190 --> 01:07:48,760
The upscale and the original will not be exactly
same, but let's compare them okay.

965
01:07:48,760 --> 01:07:54,420
Let's make them, not zoomed in, okay.

966
01:07:54,420 --> 01:07:59,490
So you see, both of these are, really similar.

967
01:07:59,490 --> 01:08:02,690
A little bit loss of quality.

968
01:08:02,690 --> 01:08:10,880
Let's also try with the anime version.

969
01:08:10,880 --> 01:08:14,640
Okay, now we got anime version.

970
01:08:14,640 --> 01:08:19,460
So let's say, you want to make your images
like anime, then you can use that.

971
01:08:19,460 --> 01:08:23,060
This is extremely useful.

972
01:08:23,060 --> 01:08:26,000
You can also upscale entire folder.

973
01:08:26,000 --> 01:08:32,700
For example, i will just ctrl a select all,
then i will drag and drop them here.

974
01:08:32,700 --> 01:08:33,979
All of them is now here.

975
01:08:33,979 --> 01:08:37,329
Now i can upscale all of them at once.

976
01:08:37,330 --> 01:08:38,330
Let me show.

977
01:08:38,330 --> 01:08:45,120
During the operation you will see they are
getting tiled like this to generate bigger

978
01:08:45,120 --> 01:08:47,770
size images.

979
01:08:47,770 --> 01:08:52,790
The results of upscaling the extras tab actually
will be inside another folder.

980
01:08:52,790 --> 01:08:58,979
When i click it, you will see they are getting
here and all of these images are now upscaled.

981
01:08:58,979 --> 01:09:04,468
For example, let's open this: this is a pixar
style image, actually.

982
01:09:04,469 --> 01:09:13,719
Okay, this is another pixar style image, so,
for example, this is also another pixar style

983
01:09:13,719 --> 01:09:17,220
image, as you can see.

984
01:09:17,220 --> 01:09:23,779
I have trained these on the Google Colab and
now i will show you how you can upload your

985
01:09:23,779 --> 01:09:31,988
model to the Google Colab and generate images
there with faster than probably your gpu,

986
01:09:31,988 --> 01:09:40,108
because the Google Colab gpu is really strong,
able to process a lot of images at once in

987
01:09:40,109 --> 01:09:41,729
a parallel way.

988
01:09:41,729 --> 01:09:52,759
Okay, you see, all of these are getting upscaled,
okay, um, let's see some of them like this,

989
01:09:52,760 --> 01:10:02,410
as you can see, okay, okay.

990
01:10:02,410 --> 01:10:06,700
Now i will show one another cool thing.

991
01:10:06,700 --> 01:10:13,270
Usually you may not get very good looking
eyes or some errors in the face, and there

992
01:10:13,270 --> 01:10:20,320
is a very good way to improve the eyes or
the overall structure of the face.

993
01:10:20,320 --> 01:10:26,830
It uses another AI model and let's try this
image improving.

994
01:10:26,830 --> 01:10:31,360
Usually, the my images were really good eyes.

995
01:10:31,360 --> 01:10:33,190
Okay, to test it.

996
01:10:33,190 --> 01:10:38,070
I am just going to not upscale, but i am going
to use GFPGAN.

997
01:10:38,070 --> 01:10:42,910
So this GFPGAN is a model to improve the eyes.

998
01:10:42,910 --> 01:10:43,910
Let's test it.

999
01:10:43,910 --> 01:10:48,190
When the first time you use it, it will download
the necessary model.

1000
01:10:48,190 --> 01:10:50,290
Okay, now let's compare the result.

1001
01:10:50,290 --> 01:10:53,770
This is the original image and this is the
fixed image.

1002
01:10:53,770 --> 01:10:59,140
Now let's also apply an upscale, okay.

1003
01:10:59,140 --> 01:11:07,600
Okay, after applying upscale and applying
a GFPGAN, you see it is now looking much better

1004
01:11:07,600 --> 01:11:09,909
in terms of quality correctness.

1005
01:11:09,909 --> 01:11:13,010
This will seriously improve the eyes.

1006
01:11:13,010 --> 01:11:14,770
Let's open them like this.

1007
01:11:14,770 --> 01:11:16,960
Okay, let's zoom in.

1008
01:11:16,960 --> 01:11:22,340
So you see the difference is huge: much better
quality, styling.

1009
01:11:22,340 --> 01:11:26,540
You can apply this to your generated images
as a batch as well.

1010
01:11:26,540 --> 01:11:31,090
Just go to batch process and select the options
from here and it will do everything.

1011
01:11:31,090 --> 01:11:33,440
You can also try these other options.

1012
01:11:33,440 --> 01:11:39,320
I didn't find them very useful actually, and
there is also not a description to them.

1013
01:11:39,320 --> 01:11:44,130
Okay, now i will show you how you can continue
training from any checkpoint that you did

1014
01:11:44,130 --> 01:11:45,380
set.

1015
01:11:45,380 --> 01:11:49,790
Just go to the search checkpoint and you will
see your saved checkpoints here, by the way,

1016
01:11:49,790 --> 01:11:57,071
to get them saved in the saving, you need
to check this generator ckpt file when saving

1017
01:11:57,071 --> 01:12:03,909
during checkpoint and then, if you generate
a new model from that checkpoint, you will

1018
01:12:03,909 --> 01:12:08,360
basically continue training from that certain
checkpoint.

1019
01:12:08,360 --> 01:12:16,790
Now i will show you how you can use these
ckpt files directly in a Google Colab.

1020
01:12:16,790 --> 01:12:22,360
If you have watched my previous video about
transform yourself into a stunning ai avatar,

1021
01:12:22,360 --> 01:12:30,360
this tutorial is how to do training on a Google
Colab and, everything is explained there to

1022
01:12:30,360 --> 01:12:34,270
use your ckpt file in a Google Colab.

1023
01:12:34,270 --> 01:12:35,270
It is so, so easy.

1024
01:12:35,270 --> 01:12:41,280
First we are going to generate a new model
from the our wanted checkpoint.

1025
01:12:41,280 --> 01:12:47,460
That let's say i want to use step 1380 as
a checkpoint.

1026
01:12:47,460 --> 01:12:52,020
Then i am giving it a name as a Colab image.

1027
01:12:52,020 --> 01:12:54,380
Okay, and nothing else.

1028
01:12:54,380 --> 01:12:56,160
Just click create model.

1029
01:12:56,160 --> 01:13:02,389
Okay, it has generated a, generated a new
model for the Colab image and inside working

1030
01:13:02,389 --> 01:13:10,480
directory you just need to upload this into
google google drive and then just give its

1031
01:13:10,480 --> 01:13:11,480
path.

1032
01:13:11,480 --> 01:13:15,300
So for i will say that, my image.

1033
01:13:15,300 --> 01:13:17,100
Okay.

1034
01:13:17,100 --> 01:13:26,120
Let's say, let's also add our keyword to that
and let's move them inside here and then go

1035
01:13:26,120 --> 01:13:34,730
to your drive folder like this, where you
are running your DreamBooth or the Stable

1036
01:13:34,730 --> 01:13:41,830
Diffusion, then drag and drop this directory
here.

1037
01:13:41,830 --> 01:13:45,180
It will upload all of the files, as you can
see in here.

1038
01:13:45,180 --> 01:13:52,159
Once the upload is completed, all we need
to do is changing model path in the inference

1039
01:13:52,159 --> 01:13:55,790
tab of the Google Colab notebook.

1040
01:13:55,790 --> 01:13:59,989
This is linked in the description of the tutorial.

1041
01:13:59,989 --> 01:14:06,219
So you need to change it like this: content:
drive my drive, and in here my drive image

1042
01:14:06,219 --> 01:14:13,510
ohwx, which is the folder name that i have
given and i am uploading to the main folder

1043
01:14:13,510 --> 01:14:15,540
of my Google Drive.

1044
01:14:15,540 --> 01:14:21,630
Then, in the Google Colab, you will be able
to use your trained ckpt file right away.

1045
01:14:21,630 --> 01:14:23,230
So what if?

1046
01:14:23,230 --> 01:14:27,660
If you want to teach another face?

1047
01:14:27,660 --> 01:14:34,260
Just generate a new model like this and this
time, in the concepts folder, set the directory

1048
01:14:34,260 --> 01:14:38,300
and the classification directory for your
new subject.

1049
01:14:38,300 --> 01:14:41,060
However, be careful with something.

1050
01:14:41,060 --> 01:14:48,520
Currently, my model is trained with ohwx man
as an instance prompt and photo of man as

1051
01:14:48,520 --> 01:14:50,050
class prompt.

1052
01:14:50,050 --> 01:14:56,969
So if i am going to teach another, a person,
a male, then i have to pick another keyword,

1053
01:14:56,969 --> 01:15:06,300
for example ske or another rare keyword, and
um, it will teach this man into the model

1054
01:15:06,300 --> 01:15:07,300
as well.

1055
01:15:07,300 --> 01:15:09,440
So we will be able to use both of them.

1056
01:15:09,440 --> 01:15:17,139
However, probably you will get mixed results
because man keyword were already taught for

1057
01:15:17,139 --> 01:15:24,000
my own image and when i introduce another
man image they will get mixed.

1058
01:15:24,000 --> 01:15:27,931
So it could be a problem, but you can try
it.

1059
01:15:27,931 --> 01:15:31,710
Test it and if you generate sufficient of
images, then i think you will.

1060
01:15:31,710 --> 01:15:34,050
You can obtain still good results.

1061
01:15:34,050 --> 01:15:42,300
However, if you inject some another class,
like a woman, then it shouldn't be much problem

1062
01:15:42,300 --> 01:15:47,080
and you should be able to teach multiple different
subjects easily.

1063
01:15:47,080 --> 01:15:50,880
Now i will explain more advanced stuff.

1064
01:15:50,880 --> 01:15:54,390
For example, the directories, data set directory.

1065
01:15:54,390 --> 01:16:02,550
Okay to be able to use [filewords], you need
to have a training data set named like this:

1066
01:16:02,550 --> 01:16:03,550
okay.

1067
01:16:03,550 --> 01:16:08,230
So for each image, you are also going to have
a text file with the same name.

1068
01:16:08,230 --> 01:16:13,610
The extension will be txt, like this, and,
you need to write the description of that

1069
01:16:13,610 --> 01:16:16,620
file properly.

1070
01:16:16,620 --> 01:16:20,660
There is a new AI model for captioning images.

1071
01:16:20,660 --> 01:16:25,590
This is not implemented to Automatic1111 yet,
but i will it will be.

1072
01:16:25,590 --> 01:16:28,250
I will put the link of this into the description.

1073
01:16:28,250 --> 01:16:31,120
You can also locally run this.

1074
01:16:31,120 --> 01:16:38,020
And if you don't know how to locally run run
this, then you need to watch our this video

1075
01:16:38,020 --> 01:16:39,219
on our channel.

1076
01:16:39,219 --> 01:16:45,620
In this video, i am explaining how to locally
run HuggingFace files.

1077
01:16:45,620 --> 01:16:51,410
Okay, and i will just use the online demo
right now because it is not very much used.

1078
01:16:51,410 --> 01:16:56,310
So, first image, i will just drag and drop
here.

1079
01:16:56,310 --> 01:16:58,410
Sorry about that.

1080
01:16:58,410 --> 01:17:01,150
Okay, like this: and click submit.

1081
01:17:01,150 --> 01:17:03,750
It will generate the description for this
image.

1082
01:17:03,750 --> 01:17:08,380
You see, you should use the caption generated
by GIT large.

1083
01:17:08,380 --> 01:17:09,770
This is the best one.

1084
01:17:09,770 --> 01:17:13,550
A man with dark hair and glasses is smiling.

1085
01:17:13,550 --> 01:17:18,690
Okay, so let's just change this text text.

1086
01:17:18,690 --> 01:17:20,139
Text description, like this.

1087
01:17:20,139 --> 01:17:27,150
However, there is one key issue: you have
to have your class for this image inside this

1088
01:17:27,150 --> 01:17:28,150
description.

1089
01:17:28,150 --> 01:17:31,210
So my class is man and therefore it is there.

1090
01:17:31,210 --> 01:17:32,239
Okay, let's go.

1091
01:17:32,239 --> 01:17:33,239
Then.

1092
01:17:33,239 --> 01:17:40,560
This is another image that we want to caption,
so let's submit it.

1093
01:17:40,560 --> 01:17:44,520
Okay, and then another image description is
here.

1094
01:17:44,520 --> 01:17:50,620
Let's open the description: a cat with long
whiskers looking at the camera.

1095
01:17:50,620 --> 01:17:54,330
And this is the class of cat, and it is inside
here as well.

1096
01:17:54,330 --> 01:17:58,830
Yes, correct, and the rest will be for dog
as well.

1097
01:17:58,830 --> 01:18:01,980
Now for classification images.

1098
01:18:01,980 --> 01:18:03,210
You need to do the same.

1099
01:18:03,210 --> 01:18:08,890
When you generate classification, you also
need to have classification image and its

1100
01:18:08,890 --> 01:18:10,960
description.

1101
01:18:10,960 --> 01:18:16,430
Let's say: this is my classification image
and it is it is generated with photo of man.

1102
01:18:16,430 --> 01:18:23,560
Therefore, i need to generate a same file
description like this and inside here i need

1103
01:18:23,560 --> 01:18:26,949
to type photo of man.

1104
01:18:26,949 --> 01:18:33,920
When this tab get fixed, let me show you maybe
it is already fixed, i am not sure.

1105
01:18:33,920 --> 01:18:40,350
In here, you see, we have generate class images
and when you use that feature, it will be

1106
01:18:40,350 --> 01:18:44,580
able to: let's try it, actually okay.

1107
01:18:44,580 --> 01:18:48,210
And let's yeah, it doesn't matter, okay.

1108
01:18:48,210 --> 01:18:56,500
And when we type class prompt here photo of
man, i think it will generate with it.

1109
01:18:56,500 --> 01:19:01,710
Let's try it, okay, it is not working.

1110
01:19:01,710 --> 01:19:06,400
It says maybe say okay, it's still not working.

1111
01:19:06,400 --> 01:19:10,130
When this become working, then you can easily
generate it.

1112
01:19:10,130 --> 01:19:17,690
Or you need to generate the description like
this: photo of man, and it will generate images

1113
01:19:17,690 --> 01:19:20,640
like that, or photo of cat or photo of dog.

1114
01:19:20,640 --> 01:19:27,500
So this will be your classification directory
with description like this and this will be

1115
01:19:27,500 --> 01:19:29,971
your classification directory with naming
like this.

1116
01:19:29,971 --> 01:19:37,429
With this way, you can teach multiple subjects
in the one run and you can also possibly improve

1117
01:19:37,429 --> 01:19:45,370
your training quality if you provide a better
description with defining more things.

1118
01:19:45,370 --> 01:19:53,130
By the way, when defining, you should specify
your subject in the description what you want

1119
01:19:53,130 --> 01:19:54,130
to teach.

1120
01:19:54,130 --> 01:19:58,610
If you want to teach face, then you should
describe the face in mostly.

1121
01:19:58,610 --> 01:20:05,080
Okay, and one another thing: okay, once you
prepared your folders.

1122
01:20:05,080 --> 01:20:08,500
Now here the way to do it.

1123
01:20:08,500 --> 01:20:14,250
First of all, we are defining the data set
directory as usual.

1124
01:20:14,250 --> 01:20:16,850
Okay, let's set it.

1125
01:20:16,850 --> 01:20:21,190
And let's also set the classification directory
like this.

1126
01:20:21,190 --> 01:20:29,230
And in [filewords], we need to use defining
prompt instance.

1127
01:20:29,230 --> 01:20:35,179
Okay, this will be used to define it.

1128
01:20:35,179 --> 01:20:37,750
It has to be a single word.

1129
01:20:37,750 --> 01:20:41,880
Therefore, i am entering ohwx and the class
token.

1130
01:20:41,880 --> 01:20:45,600
This will be also a single word.

1131
01:20:45,600 --> 01:20:55,250
By the way, it won't be very precise actually
if you use this way, class token.

1132
01:20:55,250 --> 01:21:02,600
But yeah, looks like if you teach multiple
different classes, then you may not get very

1133
01:21:02,600 --> 01:21:08,540
good performance, for example, teaching a
cat, a face, a cat, a dog and a man, because

1134
01:21:08,540 --> 01:21:12,340
they are conflicting with the current setup.

1135
01:21:12,340 --> 01:21:17,660
So using three concept is better, but let
me also explain it to you.

1136
01:21:17,660 --> 01:21:18,770
So this will be man.

1137
01:21:18,770 --> 01:21:23,470
And in prompts you are just going to type
[filewords] and class prompt.

1138
01:21:23,470 --> 01:21:30,000
You are just going to type [filewords] and
leave blank to use instance prompt optionally.

1139
01:21:30,000 --> 01:21:33,270
Use [filewords] to base sample captions on
instance images.

1140
01:21:33,270 --> 01:21:40,090
You can just also use [filewords] to see what
is what it is generating.

1141
01:21:40,090 --> 01:21:47,810
This is called mixed where in the basics of
the wiki of DreamBooth extension.

1142
01:21:47,810 --> 01:21:53,730
So you see there is DreamBooth regular training
that i have shown in this tutorial.

1143
01:21:53,730 --> 01:21:55,580
Then there is fine tuning.

1144
01:21:55,580 --> 01:21:58,030
Fine tuning is the standard approach for big
data sets.

1145
01:21:58,030 --> 01:22:00,510
Only the captions of the images are used.

1146
01:22:00,510 --> 01:22:02,920
[filewords] class images are not used.

1147
01:22:02,920 --> 01:22:07,590
These results in a model that doesn't need
instance token and reacts to any prompt.

1148
01:22:07,590 --> 01:22:10,050
So in this case you are overall training.

1149
01:22:10,050 --> 01:22:11,050
What does that mean?

1150
01:22:11,050 --> 01:22:18,260
That means that, let's say, in your [filewords]
you have cars, you have cats, you have dogs,

1151
01:22:18,260 --> 01:22:19,290
you have men.

1152
01:22:19,290 --> 01:22:22,969
You are training all of these words.

1153
01:22:22,969 --> 01:22:28,449
And this is how the custom models you see
are usually trained.

1154
01:22:28,449 --> 01:22:29,690
Let me show an example.

1155
01:22:29,690 --> 01:22:37,620
So, for example, protogen x3.4 is a custom
model and it is working pretty good.

1156
01:22:37,620 --> 01:22:38,810
How did they train it?

1157
01:22:38,810 --> 01:22:41,580
They probably trained it with fine tuning.

1158
01:22:41,580 --> 01:22:48,870
So in fine tuning they have, precisely prepared
the descriptions of each training image.

1159
01:22:48,870 --> 01:22:53,929
They didn't use any classification images
and they have overall changed the underlying

1160
01:22:53,929 --> 01:22:56,909
context, data, the knowledge of the model.

1161
01:22:56,909 --> 01:23:04,370
So when you use now man, it produces quality
of man images depending on their new fine-tuned

1162
01:23:04,370 --> 01:23:10,810
data set or car or castle or whatever that
you are improving your model on.

1163
01:23:10,810 --> 01:23:12,090
And there is hybrid.

1164
01:23:12,090 --> 01:23:15,840
Okay, actually i said mix it, but it will
be hybrid.

1165
01:23:15,840 --> 01:23:20,860
Hybrid, for lack or of better term, is achieved
using instance token in combination to [filewords]

1166
01:23:20,860 --> 01:23:21,889
as instance prompt.

1167
01:23:21,889 --> 01:23:24,900
Trained Dataset will be linked to that instance
token.

1168
01:23:24,900 --> 01:23:29,820
This minimize the bleed but requires token
in every prompt, as you can see here.

1169
01:23:29,820 --> 01:23:37,710
So you have to use or ohwx french bulldog
or ohwx, whatever you have teached.

1170
01:23:37,710 --> 01:23:39,620
Also you see the class token is person.

1171
01:23:39,620 --> 01:23:45,530
So with hybrid model with [filewords] if you,
if you don't do fine tuning but only teach

1172
01:23:45,530 --> 01:23:49,480
any subject, the subject should be, i think,
same class.

1173
01:23:49,480 --> 01:23:51,210
They can't be from different classes.

1174
01:23:51,210 --> 01:23:59,520
So you can teach multiple person in a single
run, maybe 10 person, with just providing

1175
01:23:59,520 --> 01:24:03,179
correct [filewords] and their descriptions.

1176
01:24:03,179 --> 01:24:09,540
So for this person you need to add, let's
say a man personA.

1177
01:24:09,540 --> 01:24:11,500
Okay, this will define personA.

1178
01:24:11,500 --> 01:24:16,110
For person b, you need to add personB and
for person c, you need that personC.

1179
01:24:16,110 --> 01:24:24,990
But you are not going to add into this description:
you are not going to add this instance token.

1180
01:24:24,990 --> 01:24:30,679
Okay, you don't need to type instance token
into the [filewords], into the description

1181
01:24:30,679 --> 01:24:37,560
of the training images or the into the description
of the classification images.

1182
01:24:37,560 --> 01:24:39,220
Okay, this is important.

1183
01:24:39,220 --> 01:24:46,100
Okay, now i will show how you can understand
out of memory error.

1184
01:24:46,100 --> 01:24:47,100
So it is easy.

1185
01:24:47,100 --> 01:24:50,980
I'm just going to load settings for our existing
data set.

1186
01:24:50,980 --> 01:24:52,369
You see, i have an error.

1187
01:24:52,369 --> 01:24:54,520
So it looks like i had error in cmd.

1188
01:24:54,520 --> 01:24:56,020
I just need to restart.

1189
01:24:56,020 --> 01:25:03,370
Okay, i did restart and in the settings, if
i set use EMA.

1190
01:25:03,370 --> 01:25:08,429
So actually this improves our result quality
but it costs more ram.

1191
01:25:08,429 --> 01:25:14,679
And then i just click train and let's see
how we are going to get out of memory error.

1192
01:25:14,679 --> 01:25:18,300
Okay, we got our error.

1193
01:25:18,300 --> 01:25:22,050
Let me show you how to understand out of memory
error.

1194
01:25:22,050 --> 01:25:24,790
You will see runtime CUDA out of memory.

1195
01:25:24,790 --> 01:25:28,890
If you are seeing this error, all other messages
are not important.

1196
01:25:28,890 --> 01:25:34,750
This means that with the current settings
that you are trying to training, your graphic

1197
01:25:34,750 --> 01:25:38,290
card is not enough and you need to reduce
the ram usage.

1198
01:25:38,290 --> 01:25:43,120
Now let me show you all of the settings to
how to reduce the ram usage.

1199
01:25:43,120 --> 01:25:48,980
Okay, so for minimal ram usage you need to
pick LoRA with the LoRA.

1200
01:25:48,980 --> 01:25:52,820
There is just a little bit difference.

1201
01:25:52,820 --> 01:26:00,210
It is only different when you try to do inference
and generate new images from generated LoRA

1202
01:26:00,210 --> 01:26:01,210
file.

1203
01:26:01,210 --> 01:26:06,780
And when you watch this video you will learn
that, okay, LoRA will significantly reduce

1204
01:26:06,780 --> 01:26:08,170
ram usage.

1205
01:26:08,170 --> 01:26:12,060
Other than that, always make sure that your
batch size and gradient accumulation steps

1206
01:26:12,060 --> 01:26:20,500
are one and other than that, in the advanced
tab you need to pick use 8 bit adam and select

1207
01:26:20,500 --> 01:26:23,590
bf16 and select xformers.

1208
01:26:23,590 --> 01:26:31,040
So for xformers to be able to, you need to
set your starting arguments to xformers and

1209
01:26:31,040 --> 01:26:33,300
minus minus no half.

1210
01:26:33,300 --> 01:26:35,540
These will allow you to use that.

1211
01:26:35,540 --> 01:26:36,540
Cache latents.

1212
01:26:36,540 --> 01:26:37,880
Actually, this is the.

1213
01:26:37,880 --> 01:26:39,699
This is still not clear.

1214
01:26:39,699 --> 01:26:42,830
You should try both this checked and unchecked.

1215
01:26:42,830 --> 01:26:49,179
Because some says that this increases, some
says that this decreases . So also, Step Ratio

1216
01:26:49,179 --> 01:26:50,270
of Text Encoder Training.

1217
01:26:50,270 --> 01:26:54,900
This should be zero because this increases
quality but also reduces, also increases the

1218
01:26:54,900 --> 01:26:56,900
vram usage.

1219
01:26:56,900 --> 01:27:02,040
And other than these, there is not much else
that you can do.

1220
01:27:02,040 --> 01:27:04,610
These are the lowest possible.

1221
01:27:04,610 --> 01:27:12,550
Also, you need to uncheck this checkbox and
you need to check this checkbox.

1222
01:27:12,550 --> 01:27:18,550
So when you check this checkbox it will increase
your vram usage, but when you check this checkbox

1223
01:27:18,550 --> 01:27:21,490
it will reduce your vram usage.

1224
01:27:21,490 --> 01:27:26,010
Actually, actually, the settings are written
in the troubleshooting part of the DreamBooth

1225
01:27:26,010 --> 01:27:32,300
wiki extension, in the OOM tab, and there
is also overtraining and other things.

1226
01:27:32,300 --> 01:27:37,440
Actually, overtraining is still in working
process and i have already shown you how to

1227
01:27:37,440 --> 01:27:39,469
understand overtraining.

1228
01:27:39,469 --> 01:27:45,310
And one another cool thing that i am going
to show you is preprocessing your images.

1229
01:27:45,310 --> 01:27:52,540
So with preprocessing images you can easily
generate descriptions for your both training

1230
01:27:52,540 --> 01:27:55,940
images and your classification images.

1231
01:27:55,940 --> 01:28:00,860
Of course they won't be very accurate, so
let me show you.

1232
01:28:00,860 --> 01:28:09,850
I am picking my best db 512 as source directory
and the description directory will be same.

1233
01:28:09,850 --> 01:28:16,330
So in here you can even define their target
resolution, change them, but i prefer manually

1234
01:28:16,330 --> 01:28:19,110
changing them and captioning.

1235
01:28:19,110 --> 01:28:26,640
So for captioning, i am just going to select
ignore, so it will generate new captions and

1236
01:28:26,640 --> 01:28:29,500
i am going to use deepbooru for captioning.

1237
01:28:29,500 --> 01:28:35,800
You can also generate flipped copies oversized
images, splitted, autofocal point crop.

1238
01:28:35,800 --> 01:28:41,080
So let's say you have tens of thousands of
images, then these options will be extremely

1239
01:28:41,080 --> 01:28:42,450
useful for you.

1240
01:28:42,450 --> 01:28:47,250
However, if you are only going to train your
face, then you should manually prepare your

1241
01:28:47,250 --> 01:28:53,170
training data set to be best, and then i am
going to generate captions for them.

1242
01:28:53,170 --> 01:28:54,910
I am just going to click preprocess.

1243
01:28:54,910 --> 01:29:03,000
It shouldn't change the width and height because
they are already 512 pixels and it is downloading

1244
01:29:03,000 --> 01:29:04,530
the deepbooru for captioning.

1245
01:29:04,530 --> 01:29:09,810
This is another model, just as i have shown
you in here.

1246
01:29:09,810 --> 01:29:15,750
The deepbooru is not as good as caption generated
by git large, but it is still useful and in

1247
01:29:15,750 --> 01:29:17,310
a moment we are going to see.

1248
01:29:17,310 --> 01:29:19,230
Okay, it has thrown an error.

1249
01:29:19,230 --> 01:29:22,960
Says that same director specified as source
and destination directory.

1250
01:29:22,960 --> 01:29:25,570
Obviously, this is not allowed.

1251
01:29:25,570 --> 01:29:28,610
Actually, it's a good thing that they don't
allow.

1252
01:29:28,610 --> 01:29:36,230
So i'm just going to change it as processed,
so that you don't override your original images

1253
01:29:36,230 --> 01:29:39,260
and just lets click preprocess.

1254
01:29:39,260 --> 01:29:45,690
Okay, the models are only downloaded one time,
and all images are preprocessed.

1255
01:29:45,690 --> 01:29:48,730
So let's check out the preprocessed images.

1256
01:29:48,730 --> 01:29:52,030
Okay, you see same images with descriptions.

1257
01:29:52,030 --> 01:29:53,240
Let's look at the description.

1258
01:29:53,240 --> 01:29:58,530
So the description is one: boy, black hair,
facial hair, gray pants, jacket, long sleeves,

1259
01:29:58,530 --> 01:30:05,260
male focus pants, realistic solo sub stable
track jacket and track it track pants.

1260
01:30:05,260 --> 01:30:07,119
So it's a pretty good description.

1261
01:30:07,119 --> 01:30:10,520
You can also manually modify them.

1262
01:30:10,520 --> 01:30:17,150
Let's also modify our classification images
so that, it will generate all of the description

1263
01:30:17,150 --> 01:30:18,840
of classification images.

1264
01:30:18,840 --> 01:30:22,949
By the way, this is useful, as i said, when
you use [filewords].

1265
01:30:22,949 --> 01:30:26,440
If you are not using [filewords], then these
won't get used.

1266
01:30:26,440 --> 01:30:32,119
This is also useful, very useful, if you use
a hyper network or embeddings, and i will

1267
01:30:32,119 --> 01:30:35,250
also hopefully make a video about embeddings.

1268
01:30:35,250 --> 01:30:39,179
Hyper networks are not very good, but embeddings
are really really good.

1269
01:30:39,179 --> 01:30:45,010
Okay, let's preprocess our classification
folder.

1270
01:30:45,010 --> 01:30:47,340
So the preprocess is in train tab.

1271
01:30:47,340 --> 01:30:50,230
This is a feature of Automatic1111.

1272
01:30:50,230 --> 01:30:53,409
Okay, and preprocess it.

1273
01:30:53,409 --> 01:30:57,020
It is also pretty fast.

1274
01:30:57,020 --> 01:31:00,730
So this will be extremely useful to caption.

1275
01:31:00,730 --> 01:31:07,530
And also, if your images are not properly
cropped and you have tons of thousands of

1276
01:31:07,530 --> 01:31:10,091
images, as i said, that will take huge time.

1277
01:31:10,091 --> 01:31:12,430
You can just use this.

1278
01:31:12,430 --> 01:31:17,239
As a beginner you can also use this to make
your job easier and see the results, how it

1279
01:31:17,239 --> 01:31:18,239
is performing.

1280
01:31:18,239 --> 01:31:23,460
Let's say you picked your hundreds of images
of yourself and you don't want to spend time.

1281
01:31:23,460 --> 01:31:31,810
Then you can preprocess images like this and
try, try, train, try the training on them

1282
01:31:31,810 --> 01:31:32,810
and see the results.

1283
01:31:32,810 --> 01:31:37,929
If you can get good results, then why not
spend much time, more time on them?

1284
01:31:37,929 --> 01:31:43,520
But if you want to get perfect results, then
you need to manually crop your images and

1285
01:31:43,520 --> 01:31:47,639
set your set your description.

1286
01:31:47,639 --> 01:31:49,560
So let's see the preprocess now.

1287
01:31:49,560 --> 01:31:51,840
Every image has description.

1288
01:31:51,840 --> 01:31:52,840
Let's look at them.

1289
01:31:52,840 --> 01:31:59,570
Okay, it, for example, it defined this man
as a girl, which is a very incorrect and also

1290
01:31:59,570 --> 01:32:01,050
3d asian black shirt.

1291
01:32:01,050 --> 01:32:06,219
Okay, this is a completely incorrect description,
as you can see.

1292
01:32:06,219 --> 01:32:07,909
It's completely failed.

1293
01:32:07,909 --> 01:32:12,040
And now let's compare this with the large
git which i have shown.

1294
01:32:12,040 --> 01:32:18,280
Okay, i wonder what kind of result we are
going to get with large git, so i'm just going

1295
01:32:18,280 --> 01:32:21,389
to drag and drop.

1296
01:32:21,389 --> 01:32:27,659
By the way, as i said, i have suggested adding
this model to the Automatic1111 to get better

1297
01:32:27,659 --> 01:32:32,510
results, and the large git generated a portrait
of man with beard.

1298
01:32:32,510 --> 01:32:41,350
Yes, absolutely fantastically correct when
compared to this trashy description, as you

1299
01:32:41,350 --> 01:32:42,350
can see.

1300
01:32:42,350 --> 01:32:48,320
Okay, as a final thing, i suggest you to look
at the ELI5 training.

1301
01:32:48,320 --> 01:32:55,869
So this is getting updated by the experienced
persons and, for example, in [filewords],

1302
01:32:55,869 --> 01:33:01,369
they say that they are giving an example of
instance, token alexa is bad because underlying

1303
01:33:01,369 --> 01:33:06,760
data for alexa is great and it would be hard
to override it.

1304
01:33:06,760 --> 01:33:12,700
This is also bad because this is getting split
into like this: ohwx, great.

1305
01:33:12,700 --> 01:33:15,530
Class token is also important.

1306
01:33:15,530 --> 01:33:19,400
I already experienced them, but you can also
check these pages.

1307
01:33:19,400 --> 01:33:24,170
I will put the links of these pages into the
description.

1308
01:33:24,170 --> 01:33:26,710
Now i will show you one another very cool
thing.

1309
01:33:26,710 --> 01:33:33,710
You see, this Protogen x3.4 is a custom model
that has been generated by using multiple

1310
01:33:33,710 --> 01:33:40,130
models, a lot of training, and you see, if
you train your face or subject into this model,

1311
01:33:40,130 --> 01:33:43,119
it won't produce good results.

1312
01:33:43,119 --> 01:33:48,389
Because the underlying data have been significantly
changed.

1313
01:33:48,389 --> 01:33:53,270
So how can we inject our face into this model?

1314
01:33:53,270 --> 01:33:56,230
There is a way to do that and now i am going
to show you.

1315
01:33:56,230 --> 01:34:05,020
We go to the checkpoint merger and in the
primary model we are selecting our target

1316
01:34:05,020 --> 01:34:09,090
model, which is Protogen x 3.4.

1317
01:34:09,090 --> 01:34:16,970
Secondary model will be the model that we
train it we are using, which will be this

1318
01:34:16,970 --> 01:34:19,140
one: ohwx 1308.

1319
01:34:19,140 --> 01:34:21,230
And there is tertiary model.

1320
01:34:21,230 --> 01:34:24,320
So the tertiary model will be version 1.5.

1321
01:34:24,320 --> 01:34:29,160
This is the model, this is the base model
of our model, and what we are going to do

1322
01:34:29,160 --> 01:34:37,310
is we are going to extract our image from
base model and we will apply our image into

1323
01:34:37,310 --> 01:34:39,850
the our new target model.

1324
01:34:39,850 --> 01:34:49,520
Let's give it a name: ohwx, protogen okay,
3.4, and set the weight 0.75.

1325
01:34:49,520 --> 01:34:50,520
This is 75%.

1326
01:34:50,520 --> 01:34:53,619
You may ask: how did you come up with this
value?

1327
01:34:53,619 --> 01:35:00,050
I asked the community and, according to the
experience of the community, 75% is a good

1328
01:35:00,050 --> 01:35:01,050
point.

1329
01:35:01,050 --> 01:35:03,060
You can, of course, try multiple different
points.

1330
01:35:03,060 --> 01:35:08,010
You can try your different checkpoints to
see how you perform.

1331
01:35:08,010 --> 01:35:10,410
Also, click the add difference.

1332
01:35:10,410 --> 01:35:16,230
So this will extract our face information
from our base model and it will inject our

1333
01:35:16,230 --> 01:35:25,690
face information into our new target model
without breaking the underlying context, the

1334
01:35:25,690 --> 01:35:26,690
information.

1335
01:35:26,690 --> 01:35:31,570
We are going to generate ckpt add difference
and just click run.

1336
01:35:31,570 --> 01:35:36,750
In the cmd window you will see the messages
like this: and checkpoint saved, then refresh

1337
01:35:36,750 --> 01:35:42,940
here and just go to our new model, which is
ohwx protogen.

1338
01:35:42,940 --> 01:35:51,350
Now we can produce images by using the protogen
model and our face, same as usual.

1339
01:35:51,350 --> 01:35:58,219
Okay, everyone, i have done a few tests and
the results are just amazing.

1340
01:35:58,219 --> 01:36:03,290
So you see, these are some of the images that
i have selected from the results.

1341
01:36:03,290 --> 01:36:05,469
And let me show you something.

1342
01:36:05,469 --> 01:36:12,500
So you see, this is generated by protogen
and this is my original, real image.

1343
01:36:12,500 --> 01:36:14,600
And this is the generated image.

1344
01:36:14,600 --> 01:36:15,670
You see the quality.

1345
01:36:15,670 --> 01:36:17,830
It is just amazing.

1346
01:36:17,830 --> 01:36:20,290
And what kind of test i did.

1347
01:36:20,290 --> 01:36:29,449
For testing, i have used the x/y plot, i have
entered different x values as CFG and i have

1348
01:36:29,449 --> 01:36:32,409
entered prompt sr as the weights.

1349
01:36:32,409 --> 01:36:33,860
So how did i make?

1350
01:36:33,860 --> 01:36:41,630
So you you see the ohwx man and then we are
entering a weight here, right to give an importance

1351
01:36:41,630 --> 01:36:42,630
to it.

1352
01:36:42,630 --> 01:36:49,010
So i have entered a keyword here change weight
and i have used it as a change weight here

1353
01:36:49,010 --> 01:36:50,150
in the prompt sr.

1354
01:36:50,150 --> 01:36:59,420
So the you the Automatic1111 ui into application
changed the weight for me and tested different

1355
01:36:59,420 --> 01:37:00,420
weights.

1356
01:37:00,420 --> 01:37:09,520
Now i can see the properties of this generated
part particular image to see what were the

1357
01:37:09,520 --> 01:37:10,520
used values.

1358
01:37:10,520 --> 01:37:14,179
Then, based on that, i can generate anything
i want.

1359
01:37:14,179 --> 01:37:20,810
So the weight used was 1.4 and the cfg scale
was 8.

1360
01:37:20,810 --> 01:37:28,449
So by using 1.4 and cfg scale 8 i can generate
much more quality images.

1361
01:37:28,449 --> 01:37:35,580
So these two parameters will work with my
merged model.

1362
01:37:35,580 --> 01:37:39,869
By the way, i also have used something else.

1363
01:37:39,869 --> 01:37:46,380
You see, there is a model hash and that hash,
the hash written here, also displayed here.

1364
01:37:46,380 --> 01:37:53,670
This 95 means that i have generated another
checkpoint, but this time i have used 95%

1365
01:37:53,670 --> 01:37:54,670
weight.

1366
01:37:54,670 --> 01:37:57,520
This worked better for me.

1367
01:37:57,520 --> 01:38:03,889
So in the beginning you can start with 75
and if you are not getting good images then

1368
01:38:03,889 --> 01:38:11,400
you can increase it and make different model
merges and then do test on them.

1369
01:38:11,400 --> 01:38:17,360
So this is the way how to test and find out
the good working parameters for your model

1370
01:38:17,360 --> 01:38:22,680
and then use those parameters to generate
more stylized images as you want.

1371
01:38:22,680 --> 01:38:25,070
But the results are just simply amazing.

1372
01:38:25,070 --> 01:38:30,969
You can't just get these results so easily
on the default Stable Diffusion model.

1373
01:38:30,969 --> 01:38:37,740
So you can inject your trained model, trained
face, into any custom model out there and

1374
01:38:37,740 --> 01:38:42,159
generate the beautiful images as you want.

1375
01:38:42,159 --> 01:38:45,340
So let's also upscale this image.

1376
01:38:45,340 --> 01:38:51,900
To do that, i am just going to send it to
extras and i will upscale it with R-ESRGAN

1377
01:38:51,900 --> 01:38:53,130
4x+.

1378
01:38:53,130 --> 01:38:56,739
And here the result: it is just beautiful.

1379
01:38:56,739 --> 01:39:02,340
Let's also apply GFPGAN to get better face
quality.

1380
01:39:02,340 --> 01:39:07,110
Okay, now, amazing, as you can see, amazing
quality, amazing image.

1381
01:39:07,110 --> 01:39:13,190
There is only only an artefact here, as you
can see, . So if i would generate such images,

1382
01:39:13,190 --> 01:39:15,600
i could also get rid of this artefact.

1383
01:39:15,600 --> 01:39:19,230
I think i have covered pretty much everything.

1384
01:39:19,230 --> 01:39:22,889
As i said in the beginning, just join our
discord channel.

1385
01:39:22,889 --> 01:39:29,700
From our about page and also in the in here
you will see the link.

1386
01:39:29,700 --> 01:39:35,590
Just click the official discord . Please also
share, like, subscribe, and if you support

1387
01:39:35,590 --> 01:39:39,810
us on our patreon, i would be greatly appreciated.

1388
01:39:39,810 --> 01:39:42,040
Currently we have three patrons.

1389
01:39:42,040 --> 01:39:48,159
I think i thank a lot to them for becoming
patron of our, supporting our job.

1390
01:39:48,159 --> 01:39:53,369
You can also join our channel and support
us from here, as you can see.

1391
01:39:53,369 --> 01:39:56,260
I would appreciate every bit of your support.

1392
01:39:56,260 --> 01:39:59,290
Hopefully see you in another video.

1393
01:39:59,290 --> 01:40:02,530
Please leave comments and ask the questions.

1394
01:40:02,530 --> 01:40:07,340
Ask the topics that you want to see as a new
um tutorial.

1395
01:40:07,340 --> 01:40:09,090
Thank you very much.

1396
01:40:09,090 --> 01:40:09,999
Hopefully see you later.

