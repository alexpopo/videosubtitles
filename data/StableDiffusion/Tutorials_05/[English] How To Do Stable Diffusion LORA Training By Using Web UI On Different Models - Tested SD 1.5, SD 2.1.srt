1
00:00:00,900 --> 00:00:05,160
Greetings everyone. Welcome to the 
most beginner friendly guide for how to 

2
00:00:05,160 --> 00:00:10,140
do training on Stable Diffusion 
models by using automatic1111 web UI.

3
00:00:10,140 --> 00:00:15,900
In this tutorial I will train portrait images 
of my brother by using Low-Rank Adaptation, 

4
00:00:15,900 --> 00:00:22,440
as known as LoRA training method on the 
stable diffusion 2.1 768 pixels model.

5
00:00:22,440 --> 00:00:26,940
If you do not have prior knowledge, please
watch these two videos on our channel.

6
00:00:27,480 --> 00:00:31,440
On our channel, go to the 
playlist section and in here 

7
00:00:31,440 --> 00:00:35,820
you see we have stable 
diffusion dreamboot playlist

8
00:00:35,820 --> 00:00:44,100
And in here first watch easiest way to install
and run stable diffusion web UI on PC.

9
00:00:44,100 --> 00:00:50,280
So this will teach you how to install
web UI on PC and how to run it.

10
00:00:50,280 --> 00:00:56,940
And then watch how to use stable diffusion version
2.1 and different models in the web UI.

11
00:00:56,940 --> 00:01:02,460
This will teach you how to download and install
different models and use them with the web UI.

12
00:01:02,460 --> 00:01:06,660
After that, you are ready to watch
this tutorial and follow me.

13
00:01:07,500 --> 00:01:11,580
To be able to train with LoRA, 
you need to go to the extensions 

14
00:01:11,580 --> 00:01:15,840
tab here and install DreamBooth 
extension, check for updates.

15
00:01:15,840 --> 00:01:21,240
And if you don't know how to 
install from available, first go 

16
00:01:21,240 --> 00:01:25,800
to available tab, load from 
and in here search DreamBooth.

17
00:01:26,400 --> 00:01:31,020
Since I am currently hiding extension
with installed it is not showing.

18
00:01:31,020 --> 00:01:36,960
But when I disable it, you see DreamBooth
is already installed but it has updates.

19
00:01:36,960 --> 00:01:38,580
So I am going to update it.

20
00:01:39,240 --> 00:01:43,860
OK, so for updating, we just
click apply and restart UI and it updates.

21
00:01:43,860 --> 00:01:47,160
Now, when we check for updates, you
see we have the latest version.

22
00:01:47,160 --> 00:01:50,220
And, as I said, for installation,
go to available.

23
00:01:50,220 --> 00:01:55,500
And when I check this installed, it shows
the DreamBooth is here and click install.

24
00:01:55,500 --> 00:01:56,640
OK, that's all it.

25
00:01:56,640 --> 00:02:01,740
After that, and after you 
restart your application, you may 

26
00:02:01,740 --> 00:02:05,760
need to do a full restart 
for DreamBooth tab to appear.

27
00:02:05,760 --> 00:02:07,620
You will get this tab.

28
00:02:07,620 --> 00:02:14,100
OK, and once you are in here and from the
models, you have the version 2.1.

29
00:02:15,420 --> 00:02:17,400
You are ready to follow me.

30
00:02:17,400 --> 00:02:21,000
You see, current stable diffusion
checkpoint is 2.1.

31
00:02:21,000 --> 00:02:26,340
OK, first of all, before starting our training,
we need to prepare our images.

32
00:02:26,340 --> 00:02:36,540
Since I am going to use 768 pixels version,
I need to set my images as 768 pixels.

33
00:02:36,540 --> 00:02:40,080
So my images are inside in this folder.

34
00:02:40,080 --> 00:02:43,740
I didn't still set their resolution.

35
00:02:43,740 --> 00:02:47,100
So first I will show you 
how you can crop them with 

36
00:02:47,100 --> 00:02:50,820
an open source, a free software: Paint .NET.

37
00:02:50,820 --> 00:02:54,060
Let me show you Paint .NET.

38
00:02:54,060 --> 00:02:59,280
OK, this is the Paint .NET and you can download
paint dot net from its official website in here.

39
00:02:59,280 --> 00:03:01,980
It's an open source .NET
based software.

40
00:03:01,980 --> 00:03:07,140
Alternatively, you can use this website, which
is free to resize and crop your images.

41
00:03:07,140 --> 00:03:08,880
But I prefer Paint .NET.

42
00:03:08,880 --> 00:03:12,000
I will show how to crop one of the images.

43
00:03:12,000 --> 00:03:14,820
So I am going to drag and drop
this image into here.

44
00:03:14,820 --> 00:03:16,020
Click open.

45
00:03:16,020 --> 00:03:19,020
And in here you see there
is rectangle select,

46
00:03:19,020 --> 00:03:21,420
and in here I click fixed ratio.

47
00:03:21,420 --> 00:03:24,540
I set it one hundred and
one hundred, like this.

48
00:03:24,540 --> 00:03:27,900
Then I am selecting the image
like this as I want.

49
00:03:27,900 --> 00:03:33,420
I click ctrl-C to copy, I
click ctrl-R for resize.

50
00:03:33,420 --> 00:03:37,140
I am typing something smaller
and clicking enter.

51
00:03:37,140 --> 00:03:40,260
Then I am clicking ctrl-V expanding.

52
00:03:40,260 --> 00:03:43,620
Then I am clicking ctrl-R again to resize.

53
00:03:43,620 --> 00:03:48,660
And I am exactly resizing as 768 pixels.

54
00:03:48,660 --> 00:03:53,040
Then I save it with one hundred
percent quality.

55
00:03:53,040 --> 00:03:57,000
You can also use PNG or JPG images.

56
00:03:58,440 --> 00:04:01,440
Alternatively, you can use
Birme dot net as well.

57
00:04:01,440 --> 00:04:05,760
However, you may not trust this website.

58
00:04:05,760 --> 00:04:06,960
It is up to you.

59
00:04:06,960 --> 00:04:10,020
So I will select two images
from here, upload them.

60
00:04:10,020 --> 00:04:15,780
And in here I am going to select 768
pixels like this: 768 pixels.

61
00:04:15,780 --> 00:04:22,019
OK, and you can just set where
you want cut to be like this.

62
00:04:22,019 --> 00:04:25,500
Then you need to click save as zip.

63
00:04:25,500 --> 00:04:28,320
It will download a zip like this.

64
00:04:28,320 --> 00:04:36,660
You can click it and you can then extract them
into your folder and overwrite existing files

65
00:04:36,660 --> 00:04:41,580
And they will be exactly as.
Let me show you 768 pixels.

66
00:04:41,580 --> 00:04:43,680
I will open with paint, not net,

67
00:04:43,680 --> 00:04:47,640
And you see they are 768 pixels.

68
00:04:47,640 --> 00:04:50,040
So this is the way you need
to prepare your images.

69
00:04:50,040 --> 00:04:56,640
OK, all images are cropped by
768 pixels and 768 pixels.

70
00:04:56,640 --> 00:04:58,980
Now we are ready to do training.

71
00:04:58,980 --> 00:05:03,420
So go to our stable diffusion web UI.

72
00:05:03,420 --> 00:05:06,120
So you may wonder what is LoRA?

73
00:05:06,120 --> 00:05:10,980
LoRA is a low-rank adaptation for faster
text to image diffusion fine tuning.

74
00:05:10,980 --> 00:05:13,080
It uses both UNET and CLIP.

75
00:05:13,080 --> 00:05:15,060
It is faster than DreamBooth.

76
00:05:15,060 --> 00:05:20,880
Also, its checkpoints are much smaller
than the full checkpoint of DreamBooth.

77
00:05:20,880 --> 00:05:27,000
When you do a checkpoint with DreamBooth,
it generates full .ckpt file.

78
00:05:27,840 --> 00:05:31,380
However, LoRA generates much smaller files.

79
00:05:31,380 --> 00:05:36,720
And when you are done, you can generate the
full checkpoint file. To do training

80
00:05:36,720 --> 00:05:41,640
we go to the DreamBooth tab here and we
first need to generate our model.

81
00:05:41,640 --> 00:05:43,560
I am going to use my brother.

82
00:05:43,560 --> 00:05:46,260
I need to select the source checkpoint.

83
00:05:46,260 --> 00:05:49,320
I have selected version 2.1 like this:

84
00:05:49,320 --> 00:05:55,140
I am using the EMA version and
I am not going to click this.

85
00:05:55,140 --> 00:05:56,700
This is not necessary.

86
00:05:56,700 --> 00:06:00,060
OK, and just click Create button.

87
00:06:01,500 --> 00:06:05,520
After you click Create button, 
you see it is downloading 

88
00:06:05,520 --> 00:06:09,000
the necessary files from the internet like this.

89
00:06:09,000 --> 00:06:11,820
So you need to wait this download.

90
00:06:11,820 --> 00:06:15,960
If you are not seeing anything on 
the web UI, always check the running 

91
00:06:15,960 --> 00:06:21,540
command line window to see 
what is happening like this:

92
00:06:22,500 --> 00:06:24,600
OK, the model has been generated.

93
00:06:24,600 --> 00:06:29,040
You see checkpoint successfully extracted
to Models DreamBooth my brother working.

94
00:06:29,040 --> 00:06:32,400
We can also check it from
our installed folder.

95
00:06:32,400 --> 00:06:35,520
Let's go to C drive and I have 
installed in StableDiffusion 

96
00:06:35,520 --> 00:06:39,540
web UI, in Models and in here in StableDiffusion.

97
00:06:39,540 --> 00:06:46,230
And now no, not in StableDiffusion in 
DreamBooth folder, and in here you see there 

98
00:06:46,230 --> 00:06:50,100
is working directory and there is my 
brother directory, as you can see.

99
00:06:50,100 --> 00:06:53,100
OK, let's return back to our interface.

100
00:06:53,100 --> 00:06:55,260
In here you see there is LoRA Weight.

101
00:06:55,260 --> 00:07:01,920
So this defines what percentage of LoRA 
Weight should be applied to the UNET 

102
00:07:01,920 --> 00:07:07,020
when training or creating a checkpoint, 
and it is same for Text Weight.

103
00:07:07,020 --> 00:07:12,300
Setting this as 1 may cause
overtraining, over tuning.

104
00:07:13,380 --> 00:07:20,280
However, since we are going to do generate 
our portrait images, our own portrait 

105
00:07:20,280 --> 00:07:24,900
images, and we are just teaching 
one face, this is fine for now.

106
00:07:24,900 --> 00:07:26,640
You can pick this half model.

107
00:07:26,640 --> 00:07:33,540
It will enable FP16 Precision, which results in a
smaller checkpoint with minimal loss in quality.

108
00:07:33,540 --> 00:07:38,580
But we don't need this for LoRA, since
the checkpoints are already low size.

109
00:07:39,300 --> 00:07:42,480
And when you click this, 
checkpoints will be saved to 

110
00:07:42,480 --> 00:07:44,880
a subdirectory in the selected checkpoints folder.

111
00:07:44,880 --> 00:07:48,240
So I am going to click training
wizard person.

112
00:07:48,240 --> 00:07:52,380
OK, and let's set up our parameters.

113
00:07:52,380 --> 00:07:53,520
This is really important.

114
00:07:53,520 --> 00:07:58,500
So how many training steps we
want to do for each image?

115
00:07:58,500 --> 00:08:00,420
How many images do I have?

116
00:08:00,420 --> 00:08:05,220
I have images. Let me show
once again: total: 16.

117
00:08:05,220 --> 00:08:13,380
OK, since I am going to compare checkpoints 
quality, I am going to set this very high because 

118
00:08:13,380 --> 00:08:20,280
I will early terminate the training or I will 
decide whether I have trained enough or not.

119
00:08:20,280 --> 00:08:24,180
OK, I am setting max training 
steps as zero, pause after 

120
00:08:24,180 --> 00:08:28,620
an epochs zero, amount of 
time passed between epochs.

121
00:08:28,620 --> 00:08:33,960
By the way, one epoch equal to 16
steps because I have 16 images.

122
00:08:33,960 --> 00:08:39,780
OK, and I am not going to set
any with pause between epochs.

123
00:08:39,780 --> 00:08:43,260
So use lifetime steps, epochs when saving.

124
00:08:43,260 --> 00:08:50,400
Let's say you have stopped or paused your training
and then later at a time you continue it.

125
00:08:50,400 --> 00:08:58,080
So use lifetime means that it will consider your
previous training epochs steps as well.

126
00:08:58,080 --> 00:09:02,160
However, if you unclick 
this, it will use only this 

127
00:09:02,160 --> 00:09:06,180
session of training steps and epochs when saving.

128
00:09:06,180 --> 00:09:07,560
So I am just unclicking it.

129
00:09:08,160 --> 00:09:10,140
This is really important.

130
00:09:10,140 --> 00:09:15,540
The save checkpoint frequency
by n steps.

131
00:09:15,540 --> 00:09:19,080
OK, since I didn't click this,
it will check by n steps.

132
00:09:19,080 --> 00:09:27,840
Since I have 16 images. If I set this 16, it
will save checkpoint after each epoch.

133
00:09:27,840 --> 00:09:34,200
OK, if it is confusing for you, you can just
click this and you can set this 10.

134
00:09:34,200 --> 00:09:40,380
So it will save checkpoints
every 10 epochs.

135
00:09:40,380 --> 00:09:47,400
In this case, since I have 16 images,
it will be after 160 training steps.

136
00:09:47,400 --> 00:09:48,420
OK, this is fine.

137
00:09:48,420 --> 00:09:53,820
I will also save a preview of 
the image after each checkpoint 

138
00:09:53,820 --> 00:09:58,800
so that I can decide whether 
that checkpoint is good or not.

139
00:09:58,800 --> 00:10:03,420
I will explain this in the video,
so don't worry about that,

140
00:10:03,420 --> 00:10:04,500
You will understand it.

141
00:10:04,500 --> 00:10:05,580
Batch size:

142
00:10:05,580 --> 00:10:10,380
OK, how many images to process process
at once per training step?

143
00:10:10,920 --> 00:10:16,740
We are going to process one image 
per training step and we will do 

144
00:10:16,740 --> 00:10:20,940
same for classifier regularization 
images to generate at once.

145
00:10:20,940 --> 00:10:28,740
If you have more than one GPU, you can increase
batch size to process them in parallel

146
00:10:28,740 --> 00:10:32,580
I suppose. Learning rate and other rates.

147
00:10:32,580 --> 00:10:39,660
I am not going to touch them, but you can try to
obtain better learning rates or encoder rates.

148
00:10:39,660 --> 00:10:46,680
You can also scale the learning rate, but
I am just leaving them as default.

149
00:10:46,680 --> 00:10:48,420
OK, image processing:

150
00:10:48,420 --> 00:10:49,140
This is important.

151
00:10:49,140 --> 00:10:57,780
Since I am using 768 pixel version,
I am setting it as 768.

152
00:10:57,780 --> 00:11:06,780
However, if you use another version, like version
1.5, in that case you need to use 512 pixels.

153
00:11:06,780 --> 00:11:12,540
So this resolution depends on your Stable
Diffusion model, version and type.

154
00:11:13,380 --> 00:11:19,980
I am not going to do any cropping since
I have cropped. Apply horizontal flip.

155
00:11:19,980 --> 00:11:26,520
It means that the images will be flipped as well,
so it will add more variation to your images.

156
00:11:27,240 --> 00:11:28,740
You can set this.

157
00:11:29,580 --> 00:11:31,620
Do we have pretrained VAE Name or Path?

158
00:11:31,620 --> 00:11:32,460
No, we don't have.

159
00:11:32,460 --> 00:11:35,820
We will use the base model vae.

160
00:11:35,820 --> 00:11:40,680
When you watch my previous videos, you will
learn what is vae and how to set them.

161
00:11:40,680 --> 00:11:42,540
OK, concept list:

162
00:11:42,540 --> 00:11:45,180
I am not going to use any 
concept list as well, since I am 

163
00:11:45,180 --> 00:11:51,720
just going to train for 
teaching one portrait image.

164
00:11:51,720 --> 00:11:53,280
And advanced tab:

165
00:11:53,280 --> 00:11:54,600
OK, this is important.

166
00:11:54,600 --> 00:11:57,180
This is where we set our
training methodology.

167
00:11:57,180 --> 00:12:00,120
We are going to use LoRA methodology.

168
00:12:00,120 --> 00:12:02,280
So use 8bit Adam.

169
00:12:02,280 --> 00:12:05,520
This is, enable this to save VRAM.

170
00:12:05,520 --> 00:12:10,980
If your graphic card VRAM is 
not much or let's say you have 

171
00:12:10,980 --> 00:12:14,700
encountered not enough VRAM 
problem while training,

172
00:12:14,700 --> 00:12:15,900
you can set this.

173
00:12:15,900 --> 00:12:19,860
I am going to set this for now because I'm
not sure how much VRAM it is going to take.

174
00:12:19,860 --> 00:12:23,340
And you can also set FP 16.

175
00:12:24,360 --> 00:12:25,680
So, mixed precision.

176
00:12:25,680 --> 00:12:30,540
You probably want this to be FP16.

177
00:12:30,540 --> 00:12:33,900
If using Xformers. You definitely
want this to be FP 16.

178
00:12:33,900 --> 00:12:37,680
And if you have watched my previous 
videos, you know that we are 

179
00:12:37,680 --> 00:12:42,540
already using Xformers to speed 
up our inference and training.

180
00:12:43,560 --> 00:12:45,780
So I am going to set this.
Memory Attention:

181
00:12:45,780 --> 00:12:48,060
I am going to set this Xformers.

182
00:12:48,060 --> 00:12:53,160
My graphic card is RTX 3060
and it supports that.

183
00:12:53,880 --> 00:12:56,040
OK, Don't Cache Latents:

184
00:12:57,060 --> 00:13:01,020
When I hover my mouse over 
that, you see, there appears a 

185
00:13:01,020 --> 00:13:05,460
tooltip and explains to me what 
does that checkbox is doing.

186
00:13:05,460 --> 00:13:09,420
When this box is checked, latents
will not be cached.

187
00:13:09,420 --> 00:13:14,040
When latents are not caged, you will save
a bit of VRAM but train slightly slower.

188
00:13:14,040 --> 00:13:18,060
So for a lower VRAM usage,
I am checking this.

189
00:13:18,060 --> 00:13:21,420
Train Text Encode. Enabling 
this will provide better 

190
00:13:21,420 --> 00:13:26,640
results and editability but cost more VRAM.

191
00:13:26,640 --> 00:13:28,560
Yes, we are setting this.

192
00:13:28,560 --> 00:13:33,060
I am not changing any
of default parameters

193
00:13:33,780 --> 00:13:36,360
And I am not changing these
parameters as well.

194
00:13:36,360 --> 00:13:39,720
So we are done with parameters tab.

195
00:13:40,920 --> 00:13:43,440
By the way, I will make 
this arbitrarily high number 

196
00:13:43,440 --> 00:13:46,200
because I will stop it 
myself. The training myself.

197
00:13:47,460 --> 00:13:48,720
OK, concepts.

198
00:13:48,720 --> 00:13:49,860
This is really important.

199
00:13:49,860 --> 00:13:55,680
This is the part where we are
setting what to teach.

200
00:13:55,680 --> 00:13:59,100
OK, maximum training steps is minus one.

201
00:13:59,100 --> 00:14:00,900
It will never end.

202
00:14:00,900 --> 00:14:06,540
OK, data set directory. Path to the
directory with input images.

203
00:14:07,200 --> 00:14:11,340
So to get the director of my 
input images. I am right clicking 

204
00:14:11,340 --> 00:14:14,460
one of the image, right clicking 
properties, left clicking.

205
00:14:14,460 --> 00:14:16,920
And in here you see it shows location.

206
00:14:16,920 --> 00:14:19,200
I am copying this like this.

207
00:14:19,200 --> 00:14:26,760
Alternatively, you can also click the search
bar here and select entire path and copy.

208
00:14:27,480 --> 00:14:29,040
And I am pasting it here.

209
00:14:29,040 --> 00:14:31,620
Classification data set directory.

210
00:14:32,160 --> 00:14:36,540
OK, this is a path to directly with classification
regularization images.

211
00:14:36,540 --> 00:14:42,240
So let's also set a path for this
to understand what is it.

212
00:14:42,240 --> 00:14:48,120
So, however, you shouldn't set them 
inside here because in that case 

213
00:14:48,120 --> 00:14:52,260
it is using, I think, all of the 
images in all of the folders.

214
00:14:52,260 --> 00:14:55,320
So let's say, brother

215
00:14:57,720 --> 00:14:59,580
classification folder.

216
00:15:00,840 --> 00:15:02,460
OK, let's enter here.

217
00:15:02,460 --> 00:15:03,840
Copy the path like this.

218
00:15:05,280 --> 00:15:12,180
File words: OK, we are not going to 
use any file words in this training 

219
00:15:12,180 --> 00:15:18,900
because we are not training hypernet, 
or let me show you what was it.

220
00:15:18,900 --> 00:15:21,180
It is embedding.

221
00:15:21,180 --> 00:15:23,760
Therefore, you can just leave this empty.

222
00:15:23,760 --> 00:15:30,180
But prompts: this is where we need 
to enter a unique prompt to teach 

223
00:15:30,180 --> 00:15:36,120
the face of or the other thing 
that we want to teach to the model.

224
00:15:36,840 --> 00:15:41,820
So I will give it a unique
name as my brother.

225
00:15:41,820 --> 00:15:45,060
OK, like this, you can
give any unique name.

226
00:15:45,060 --> 00:15:51,720
It should be unique enough that it won't
be in the original training data set.

227
00:15:51,720 --> 00:15:56,700
So to be sure, you can just also expand
it like this. Class Prompt. Now

228
00:15:56,700 --> 00:15:57,540
this is important.

229
00:15:57,540 --> 00:15:59,940
What am I teaching to the model?

230
00:15:59,940 --> 00:16:01,920
I am teaching face of a man.

231
00:16:01,920 --> 00:16:04,680
So I will say face of a man.

232
00:16:06,420 --> 00:16:11,760
OK, like this. Classification
image, negative prompt.

233
00:16:11,760 --> 00:16:19,380
OK, you may give a negative prompt to generate
better quality classification images.

234
00:16:19,380 --> 00:16:24,420
These images will be generated to
improve your training success.

235
00:16:24,420 --> 00:16:26,040
They will be automatically generated.

236
00:16:26,040 --> 00:16:29,040
So let's enter a good
negative prompt here.

237
00:16:29,040 --> 00:16:33,600
OK, I have a decent negative prompt
I have prepared previously.

238
00:16:33,600 --> 00:16:40,380
I have used a chatGPT to expand some
of the famous negative prompts.

239
00:16:40,380 --> 00:16:44,160
I will put this into the comment
section of the video.

240
00:16:44,160 --> 00:16:46,800
So, don't worry, you will be
able to copy and paste it.

241
00:16:46,800 --> 00:16:49,440
So I'm copying and pasted in here.

242
00:16:49,440 --> 00:16:54,360
So I will explain to you
what is this as well.

243
00:16:54,360 --> 00:16:59,040
So this class prompt and classification
images what are they?

244
00:16:59,040 --> 00:17:00,900
Sample Image Prompt.

245
00:17:00,900 --> 00:17:01,740
This is important

246
00:17:01,740 --> 00:17:02,160
Why?

247
00:17:02,160 --> 00:17:07,079
Because during the training we want to
see how the training is going on.

248
00:17:07,079 --> 00:17:10,679
And for that we will generate
sample images.

249
00:17:10,680 --> 00:17:14,400
So the sample images will be like this:

250
00:17:14,400 --> 00:17:18,180
First we will give the 
instance prompt so that we will 

251
00:17:18,180 --> 00:17:24,900
be able to see the face of 
the person we are teaching.

252
00:17:24,900 --> 00:17:30,420
Then you can append here some of the good
keywords to obtain better results.

253
00:17:30,420 --> 00:17:35,400
But if you just want to see 
how much the model has learned, 

254
00:17:35,400 --> 00:17:38,580
you can leave this only with the instance prompt.

255
00:17:38,580 --> 00:17:47,460
So you will get a better idea of how much
the face has been learned by the model.

256
00:17:47,460 --> 00:17:50,040
Sample prompt template.

257
00:17:50,040 --> 00:17:52,320
We don't need a prompt template right now.

258
00:17:52,320 --> 00:17:54,240
Sample image: negative prompt.

259
00:17:54,240 --> 00:17:57,540
You can just copy and paste
it in here as well.

260
00:17:58,080 --> 00:17:59,880
Okay, Image Generation.

261
00:17:59,880 --> 00:18:07,860
So when doing training we will generate, 
let's say, generic images, generic 

262
00:18:07,860 --> 00:18:16,140
face of a man, images to make 
our training more generalized.

263
00:18:16,140 --> 00:18:19,320
Okay, to improve its success rate.

264
00:18:19,320 --> 00:18:23,880
So I will generate 10 times
of the my input like this.

265
00:18:23,880 --> 00:18:32,460
And these are for the other things 
that you need to set for generating 

266
00:18:32,460 --> 00:18:35,640
template images, we may say, 
or generic images, we may say.

267
00:18:35,640 --> 00:18:37,740
Number of samples to generate one.

268
00:18:37,740 --> 00:18:39,120
Okay, it looks good.

269
00:18:39,120 --> 00:18:41,700
Okay, you can just enter 10 times of it.

270
00:18:41,700 --> 00:18:43,680
It's up to you.

271
00:18:43,680 --> 00:18:47,880
And we are ready to do training.

272
00:18:47,880 --> 00:18:53,460
But I will also show you one
other thing in settings. In settings

273
00:18:53,460 --> 00:19:02,520
when you go to training section here, you can
reduce VRAM usage by clicking this checkbox.

274
00:19:02,520 --> 00:19:06,600
Okay, I think it will probably
reduce your training speed.

275
00:19:06,600 --> 00:19:11,040
And you can also turn on pin 
memory for data loader. Make 

276
00:19:11,040 --> 00:19:13,740
training slightly faster 
but increase memory usage.

277
00:19:13,740 --> 00:19:20,580
So you may play with these settings to obtain
the best possible training speed.

278
00:19:20,580 --> 00:19:24,960
I have 12 GB VRAM.

279
00:19:24,960 --> 00:19:29,700
So I may open this, but I won't open that.

280
00:19:29,700 --> 00:19:34,080
Also. Yeah, the others are just fine.

281
00:19:34,980 --> 00:19:40,620
Okay, and let's click training
button, since we are ready.

282
00:19:40,620 --> 00:19:50,520
Okay, when we click training button, first it will
generate the generic face of a man images.

283
00:19:50,520 --> 00:19:52,500
Why face of a man?

284
00:19:52,500 --> 00:20:01,800
Because we have entered our class prompt as
face of a man and where they will be saved.

285
00:20:01,800 --> 00:20:05,460
I think they will save it in
here brother classification.

286
00:20:07,080 --> 00:20:16,560
So it is starting to generate our generic face
images to add more variety to our data set.

287
00:20:16,560 --> 00:20:21,600
So you see, it is generating face of
a man with the given prompt I have.

288
00:20:22,200 --> 00:20:28,800
You can also improve this prompt 
with adding other, let's say, 

289
00:20:28,800 --> 00:20:31,800
styling prompts, more quality 
prompts, anything you want.

290
00:20:32,880 --> 00:20:40,560
If you watch my previous videos on the channel, on
the playlist, you will understand what I mean.

291
00:20:40,560 --> 00:20:47,640
This is actually same as doing inference text2img
from here to generate images.

292
00:20:47,640 --> 00:20:59,280
It is exactly doing that to improve variety
of our classification training.

293
00:20:59,280 --> 00:21:02,220
You see they are really
bad quality right now,

294
00:21:04,080 --> 00:21:09,840
So maybe we should tune our class prompt.

295
00:21:12,240 --> 00:21:19,020
To do that I will just cancel the training
with clicking cancel button.

296
00:21:19,020 --> 00:21:26,640
You see training cancelled. And you see that
these are the images it has generated.

297
00:21:26,640 --> 00:21:30,000
I will modify. I have deleted all of them.

298
00:21:30,000 --> 00:21:37,980
I will modify the class prompt
by adding some keywords.

299
00:21:37,980 --> 00:21:43,140
Okay, to decide what to enter, 
I have moved to text2image tab 

300
00:21:43,140 --> 00:21:49,560
and I have typed: portrait photo 
of a man, HDR, 8K and sharp.

301
00:21:50,400 --> 00:21:55,800
The keywords that you will find from 
real photos of people. And I have entered 

302
00:21:55,800 --> 00:21:59,520
my negative prompt as well and this 
is the image that has been generated.

303
00:21:59,520 --> 00:22:01,320
It is pretty good.

304
00:22:01,320 --> 00:22:06,300
So I am returning back to my DreamBooth 
tab and in here I am changing 

305
00:22:06,300 --> 00:22:13,620
to my the class prompt like this 
and now I am clicking train again.

306
00:22:14,940 --> 00:22:20,280
Now it will generate 160 class
images for training.

307
00:22:21,600 --> 00:22:28,800
Basically the generic images to
improve my training quality.

308
00:22:28,800 --> 00:22:31,020
Let's see what kind of results
we are going to get.

309
00:22:31,020 --> 00:22:35,760
We should get results same as we got
in text2image tab actually.

310
00:22:35,760 --> 00:22:40,320
Yeah, it's a decent face photo of a male.

311
00:22:40,320 --> 00:22:49,680
Why we are doing this? As I said, 
to increase the variance variation. 

312
00:22:49,680 --> 00:22:54,840
When you have different styles, 
different variations of photos.

313
00:22:54,840 --> 00:23:02,220
It will prevent over training 
and it will force model 

314
00:23:02,220 --> 00:23:07,200
to learn face of the person 
that you want to teach.

315
00:23:07,200 --> 00:23:12,540
OK, so you see, now we are getting 
really decent quality face images 

316
00:23:12,540 --> 00:23:20,160
of male persons and it will 
help our model to learn better.

317
00:23:21,060 --> 00:23:24,360
OK, meanwhile, the training is going on.

318
00:23:24,360 --> 00:23:27,960
I mean, the image generation is going on.

319
00:23:27,960 --> 00:23:29,580
Let's quickly recap.

320
00:23:29,580 --> 00:23:33,900
First, we generated our 
model with a unique name like 

321
00:23:33,900 --> 00:23:36,840
this, and we have selected the source checkpoint.

322
00:23:36,840 --> 00:23:42,540
You can source checkpoint any model on
the Internet that you want to teach.

323
00:23:42,540 --> 00:23:47,460
OK, it will work. Exactly
same as version 2.1.

324
00:23:47,460 --> 00:23:52,860
The only thing that may differ 
if your model is based on stable 

325
00:23:52,860 --> 00:24:00,780
diffusion 1.5 or 1.4, they 
use 512 pixel size images.

326
00:24:00,780 --> 00:24:11,040
Therefore, the only thing that you need to change
is the image size, which is where was it?

327
00:24:11,040 --> 00:24:16,800
Let me show you. Here image
processing resolution.

328
00:24:16,800 --> 00:24:25,200
So, if you use a checkpoint model 
based on 1.5, 1.4 or 512 pixel based 

329
00:24:25,200 --> 00:24:30,840
2.x version, then you need 
to change this to 512 pixel.

330
00:24:30,840 --> 00:24:37,200
But if you are using 2.1 version 
based model which has native 

331
00:24:37,200 --> 00:24:44,040
768 pixel resolution, then 
you need to change that.

332
00:24:44,040 --> 00:24:49,200
Other than that, we are going to parameters:
training steps per image.

333
00:24:49,200 --> 00:24:55,920
I have set this very big number because I will
stop the training myself at a certain point.

334
00:24:55,920 --> 00:25:00,840
I will show you when I will stop it and
how I will decide to stop training,

335
00:25:01,380 --> 00:25:08,760
And this is important. I will save and
generate images every 10 epoch.

336
00:25:08,760 --> 00:25:12,960
Every 10 epoch means that 
one epoch will happen when 

337
00:25:12,960 --> 00:25:16,320
it process all of the images 
in my training folder.

338
00:25:16,320 --> 00:25:21,960
I have 16 images in my training
folder, which is here.

339
00:25:21,960 --> 00:25:27,480
It will also put, I think, flipped images
there, so it will be 32 images.

340
00:25:27,480 --> 00:25:31,620
We will see that when training 
starts, currently still generating 

341
00:25:32,640 --> 00:25:35,940
the generic images that I 
have requested, like this.

342
00:25:36,840 --> 00:25:43,380
OK, so I will be able to decide 
whether model has learned enough 

343
00:25:43,380 --> 00:25:48,420
so that I can stop and start 
using the model or not.

344
00:25:48,420 --> 00:25:56,400
OK, so these save previews and save checkpoints is
really important to see the progress of training.

345
00:25:56,940 --> 00:26:03,060
The batch size is, I think, related to 
how many GPUs you have, or if you have 

346
00:26:03,060 --> 00:26:09,300
a very strong GPU that can process in 
parallel two images at the same time.

347
00:26:09,300 --> 00:26:13,440
If it has enough VRAM memory,
you can also increase this.

348
00:26:13,440 --> 00:26:18,540
But if your graphic card can 
only process one image at 

349
00:26:18,540 --> 00:26:21,840
a time, then you should 
leave both of these as one.

350
00:26:22,440 --> 00:26:29,460
I didn't change any of the learning rates or
other things. I did leave them default.

351
00:26:30,240 --> 00:26:36,240
I have also applied horizontal flip 
randomly. Decide to flip images horizontally 

352
00:26:36,240 --> 00:26:42,420
so that it will add more variation 
to the learning data set.

353
00:26:42,420 --> 00:26:46,680
I don't have any VAE or concept list.

354
00:26:46,680 --> 00:26:55,620
I am using LoRA because with this way it
will use lesser VRAM than DreamBooth.

355
00:26:55,620 --> 00:27:01,380
And the save files will be 1000 times 
lesser than the DreamBooth, because 

356
00:27:01,380 --> 00:27:06,900
Dreamboot generates full size 
model files for checkpoints.

357
00:27:06,900 --> 00:27:12,480
However, this will generate minimal 
files. Then from those files, after we 

358
00:27:12,480 --> 00:27:16,680
are satisfied with the training 
process, we will generate full model.

359
00:27:16,680 --> 00:27:22,200
I am using 8bit Adam to save 
VRAM. I am using mixed precision 

360
00:27:22,200 --> 00:27:25,920
memory attention and I didn't 
change any other parameters.

361
00:27:25,920 --> 00:27:30,900
And in the concepts I did 
set my data set directory 

362
00:27:30,900 --> 00:27:33,180
and the classification data set directory.

363
00:27:33,180 --> 00:27:36,600
I have already shown you them. 
We are not using any file words 

364
00:27:36,600 --> 00:27:41,520
because we are not doing a 
general concept training.

365
00:27:41,520 --> 00:27:47,580
It is not the context of this 
video. I may make another 

366
00:27:47,580 --> 00:27:52,320
video to train hypernet or textual embeddings.

367
00:27:52,320 --> 00:27:55,380
The instance prompt. This 
is really important because 

368
00:27:55,380 --> 00:28:02,280
this keyword is being taught to our model.

369
00:28:02,280 --> 00:28:09,420
So when I do inference with the 
new model, the tuned model, it will 

370
00:28:09,420 --> 00:28:16,140
know that this keyword is the 
face of my brother pictures.

371
00:28:17,460 --> 00:28:20,160
Therefore, this is really 
important. This is the generic 

372
00:28:20,160 --> 00:28:23,820
class prompt. I already have explained that.

373
00:28:24,540 --> 00:28:28,560
And these are the arbitrary numbers. 
Actually, this is the arbitrary 

374
00:28:28,560 --> 00:28:31,560
number I have entered. I 
didn't change the other things.

375
00:28:33,120 --> 00:28:39,360
These are only affecting the
images generated in here.

376
00:28:40,560 --> 00:28:45,660
None other than that. So this part is
only important for these images.

377
00:28:46,860 --> 00:28:52,020
So it will generate 160 images in this 
folder. You see, it is also generating 

378
00:28:52,020 --> 00:28:59,640
same named text files and it is 
saving the description of the input.

379
00:28:59,640 --> 00:29:02,700
You could also modify these 
descriptions, but I think 

380
00:29:02,700 --> 00:29:05,280
it is not very important for LoRA training.

381
00:29:05,280 --> 00:29:10,620
It is important for hypernetwork and
especially for text embeddings.

382
00:29:10,620 --> 00:29:17,340
Now I will pause video until the image generation
has been done and the training has started.

383
00:29:17,880 --> 00:29:23,940
OK, meanwhile, the class image generation:
So far, we are almost at 50 percent.

384
00:29:23,940 --> 00:29:28,680
It says there is still 20 
minutes remaining. Approximately. 

385
00:29:28,680 --> 00:29:32,640
It is using 95 percent of my GPU.

386
00:29:32,640 --> 00:29:41,400
It is using almost 9 gigabytes of my GPU
and it is using about 20 percent of CPU.

387
00:29:41,400 --> 00:29:47,040
So these are the values that it is using
for just class image generation.

388
00:29:47,040 --> 00:29:51,120
And let's see how much it
will use for training.

389
00:29:51,780 --> 00:29:58,680
And the class image generation
speed is 14.58 seconds / IT.

390
00:30:00,360 --> 00:30:04,920
OK, the training process has started.
After generating all of the images.

391
00:30:04,920 --> 00:30:09,780
Let me show you them. Once you 
have generated these generic 

392
00:30:09,780 --> 00:30:12,180
images, you don't have to 
generate them once again.

393
00:30:12,180 --> 00:30:18,480
You can stop and restart training and
use these base generic images.

394
00:30:19,320 --> 00:30:25,800
However, an error occurred. So my web interface
is not getting updated anymore,

395
00:30:25,800 --> 00:30:32,460
unfortunately. But the training
is going on, as you can see.

396
00:30:32,460 --> 00:30:39,240
Currently it is two iterations, actually
two seconds per iteration as a speed.

397
00:30:40,560 --> 00:30:46,020
It has done 145 iterations so far.

398
00:30:47,040 --> 00:30:54,300
And let's see how much VRAM. Oh, you
see, my entire VRAM is almost full.

399
00:30:55,980 --> 00:30:59,580
It says that there is 
allocated and reserved, but I 

400
00:30:59,580 --> 00:31:02,820
am seeing the full VRAM usage in my graphic card.

401
00:31:04,620 --> 00:31:13,860
And after 10 epochs we are supposed to
get our first training output to see.

402
00:31:13,860 --> 00:31:17,700
OK, it says that you see, LoRA 
weights successfully saved 

403
00:31:17,700 --> 00:31:20,700
to C stable diffusion web UI, which is my folder.

404
00:31:20,700 --> 00:31:33,180
Inside models LoRA. So let's go there and check it
out. In stable diffusion web UI in models in LoRA.

405
00:31:33,180 --> 00:31:40,500
OK, so you see, this is the checkpoint file it
has generated and it is only three megabytes.

406
00:31:40,500 --> 00:31:45,420
So I can generate checkpoint
file for even every epoch.

407
00:31:45,420 --> 00:31:50,280
However, if we were using DreamBooth 
instead of the LoRA, this would 

408
00:31:50,280 --> 00:31:55,560
be minimal, like 5 gigabytes 
or 6 gigabytes or 4 gigabytes

409
00:31:55,560 --> 00:31:59,760
based on the model that you are using
as the first initial checkpoint.

410
00:32:01,320 --> 00:32:08,640
So for version 2.1, it would 
be equal to minimum five 

411
00:32:08,640 --> 00:32:11,340
gigabytes because the base 
model is five gigabytes.

412
00:32:11,340 --> 00:32:17,400
If we were using DreamBooth instead of LoRA,
our every checkpoint would be five gigabytes.

413
00:32:17,400 --> 00:32:22,680
But now we are only getting
three megabytes checkpoint.

414
00:32:22,680 --> 00:32:25,740
It is even smaller than
1/1000.

415
00:32:26,940 --> 00:32:37,440
OK, so we should also got the first
image output of our training.

416
00:32:37,440 --> 00:32:42,480
So where it is saved, If 
you are wondering, I think 

417
00:32:42,480 --> 00:32:46,680
inside DreamBooth, in my 
brother's in here samples.

418
00:32:46,680 --> 00:32:54,720
Yes, So this is the first sample image it has
generated after ten epochs. In this folder,

419
00:32:54,720 --> 00:33:05,580
as the time passes we are going to see images that
will be similar to my brother's sample images.

420
00:33:05,580 --> 00:33:08,580
Let me show you our training
data set images.

421
00:33:09,180 --> 00:33:10,740
Let me show you once again.

422
00:33:10,740 --> 00:33:20,700
So once we get images as close as possible to our 
training data set, then we will, we will, we will 

423
00:33:20,700 --> 00:33:31,320
generate a checkpoint model file, full model 
file, from that file. Which file from the file?

424
00:33:32,100 --> 00:33:41,700
Let me open once again the C folder to explain
better. In here in models in LoRA.

425
00:33:41,700 --> 00:33:50,400
So once we got a good image we are going
to you see the file name is 160.

426
00:33:50,400 --> 00:33:56,940
We are going to get the same file in here and 
we will generate a full model checkpoint from 

427
00:33:56,940 --> 00:34:03,480
that and then we will be able to generate 
the images of the person we train it for.

428
00:34:03,480 --> 00:34:08,760
OK, so now I will pause video until
we got some good results.

429
00:34:10,500 --> 00:34:15,120
Oh, by the way, it seems 
like. Yes, yes, the process 

430
00:34:15,120 --> 00:34:17,580
has stopped. So therefore I have to continue.

431
00:34:18,480 --> 00:34:20,640
Probably an error has occurred.

432
00:34:21,960 --> 00:34:24,480
Yeah, an error occurred.

433
00:34:24,480 --> 00:34:30,600
So what we need to do is: OK, let me
show you to continue from there.

434
00:34:30,600 --> 00:34:33,840
So I am refreshing the web interface.

435
00:34:33,840 --> 00:34:36,239
This error may occur time to time

436
00:34:36,239 --> 00:34:38,999
And in here I go back to the DreamBooth.

437
00:34:39,000 --> 00:34:46,739
And in here you see, let's refresh.
We have my brother as LoRA model.

438
00:34:46,739 --> 00:34:54,359
And OK, so let's click load params
and see if it will load.

439
00:34:54,360 --> 00:34:55,800
I hope it loads.

440
00:34:57,660 --> 00:34:59,340
OK, it says loading.

441
00:34:59,340 --> 00:35:02,520
I maybe need to restart the application.

442
00:35:04,080 --> 00:35:06,480
Yeah, probably I need to
restart the application.

443
00:35:06,480 --> 00:35:13,980
You see, when you play with the web UI while
training, these kind of errors may occur.

444
00:35:13,980 --> 00:35:15,720
So I will now restart the application.

445
00:35:15,720 --> 00:35:21,000
OK, after I close the command line,
you see connection error occurred.

446
00:35:21,000 --> 00:35:25,680
So let's go back to our stable
diffusion web UI.

447
00:35:25,680 --> 00:35:28,080
Click web UI webui-user.bat.

448
00:35:29,220 --> 00:35:31,440
OK, I have restarted the web UI.

449
00:35:31,440 --> 00:35:37,800
Now refresh and go to the 
DreamBooth and pick the LoRA model.

450
00:35:37,800 --> 00:35:40,800
And let's click load params.

451
00:35:40,800 --> 00:35:42,540
Please specify model to load.

452
00:35:42,540 --> 00:35:46,860
So you see, my brother model
is now here. After restart,

453
00:35:46,860 --> 00:35:53,640
and after I click load params, 
load loaded config and I 

454
00:35:53,640 --> 00:35:56,520
click train, it should 
continue from where it is left.

455
00:35:56,520 --> 00:36:00,240
OK, you see, it is getting. Concept
requires 160 images.

456
00:36:00,240 --> 00:36:06,720
It has loaded the same images, so it is not
regenerating the classification images,

457
00:36:06,720 --> 00:36:12,600
It is loading the weights where
it is left, I think.

458
00:36:12,600 --> 00:36:16,560
So the number of examples: 16. Number
of batches per epoch: 16..

459
00:36:16,560 --> 00:36:17,280
Correct.

460
00:36:17,280 --> 00:36:25,440
Number of epochs: one million. As we have set,
the total optimization step is 16 million.

461
00:36:25,440 --> 00:36:28,560
OK, everything looks correct.

462
00:36:28,560 --> 00:36:34,320
And, yes, it is now continuing
where it is left.

463
00:36:34,320 --> 00:36:35,340
This is great.

464
00:36:35,340 --> 00:36:41,460
So if an error occurs, this is how you 
are going to continue your training.

465
00:36:42,240 --> 00:36:46,980
So you can further optimize your model 
from any checkpoint. And in here,  

466
00:36:46,980 --> 00:36:48,600
if you see, let me zoom in.

467
00:36:50,220 --> 00:36:58,020
You see, OK, I did zoom too much. Training
step is 23. This is the current session.

468
00:36:58,020 --> 00:37:00,360
And, you see, this is
the lifetime session.

469
00:37:00,360 --> 00:37:05,160
So this is different that
you are setting in here.

470
00:37:05,160 --> 00:37:07,500
Use lifetime steps, epochs when saving.

471
00:37:08,100 --> 00:37:09,000
We didn't check that.

472
00:37:09,000 --> 00:37:13,740
So we are only taking into 
account the current session 

473
00:37:13,740 --> 00:37:17,400
steps for saving and previewing the checkpoints.

474
00:37:17,400 --> 00:37:22,140
But this is the lifetime. OK, now
I will pause the video again.

475
00:37:23,640 --> 00:37:29,340
OK, so you see, after the second save
and now it continue to do training.

476
00:37:29,340 --> 00:37:34,320
So sometimes errors may happen,
even though they shouldn't.

477
00:37:35,280 --> 00:37:42,960
So if an error happens, just restart application,
just as I have shown, and continue to training.

478
00:37:42,960 --> 00:37:46,440
So the samples are getting produced.

479
00:37:46,440 --> 00:37:53,460
I hope it doesn't take too much time to
teach my brother face into the model.

480
00:37:55,200 --> 00:37:59,940
OK, it has been only 30 
epochs so far and we already 

481
00:37:59,940 --> 00:38:04,800
got somewhat similar picture in the third one.

482
00:38:04,800 --> 00:38:10,020
You see, this is the thirty epoch
after 480 steps in total.

483
00:38:10,020 --> 00:38:11,880
And this is my brother.

484
00:38:11,880 --> 00:38:16,800
You see, there is a similarity,
as you can see.

485
00:38:17,880 --> 00:38:20,220
OK, I have noticed another mistake.

486
00:38:20,220 --> 00:38:25,920
You see the command line interface 
is displaying the LoRA weights 

487
00:38:25,920 --> 00:38:31,020
has been saved to my brother, 
underline 160 dot PT.

488
00:38:31,020 --> 00:38:34,320
However, it is correctly
saving in the folder.

489
00:38:34,320 --> 00:38:40,560
So this printed message is incorrect, but the same
with file names are correct, as you can see.

490
00:38:40,560 --> 00:38:47,280
So this is the thirty epoch has been done
actually for. Actually for the epochs,

491
00:38:47,280 --> 00:38:52,560
Yes, since we have 16 images, when you
divide this 16, it is 40 epochs.

492
00:38:52,560 --> 00:38:55,440
And these are the so far generated images.

493
00:38:55,440 --> 00:38:59,820
You see it starts to resemble more
and more as the training continue.

494
00:39:00,840 --> 00:39:05,460
OK, it has been over 1423 steps so far.

495
00:39:06,180 --> 00:39:15,600
So the training step speed is, for 
1.60 seconds for per iteration.

496
00:39:15,600 --> 00:39:19,140
So far it is going on.

497
00:39:19,140 --> 00:39:24,000
We are getting closer to our target
image, as you can see here.

498
00:39:24,840 --> 00:39:25,500
OK,

499
00:39:27,060 --> 00:39:35,040
It has been over 5600 steps so
far, which makes 350 epochs.

500
00:39:35,040 --> 00:39:44,340
And for this tutorial I am now going to cancel
the training and I will generate a checkpoint

501
00:39:45,480 --> 00:39:55,920
based on the best that comes to me as best
epoch number, which is sample 2400.

502
00:39:55,920 --> 00:40:00,360
You can continue to do training until
you are satisfied with the results.

503
00:40:00,360 --> 00:40:07,740
But these results are just a preview of
what it has learned. With a good prompt

504
00:40:07,740 --> 00:40:10,800
you can obtain much better photos.

505
00:40:10,800 --> 00:40:14,700
And also it also depends on
your data set quality.

506
00:40:14,700 --> 00:40:21,420
If you prepare better images than in this example,
you can still obtain better results.

507
00:40:21,420 --> 00:40:23,880
I think this is a decent one.

508
00:40:23,880 --> 00:40:27,780
And let's generate our model checkpoint.

509
00:40:27,780 --> 00:40:32,520
So how we are going to generate our
model checkpoint to use later.

510
00:40:32,520 --> 00:40:41,580
You see there is a LoRA model and now we are
going to generate checkpoint from our 2400.

511
00:40:42,960 --> 00:40:46,200
I am entering the model name
here that I want to give.

512
00:40:46,200 --> 00:40:54,660
Let's say, my brother test one, and
I am clicking generate ckpt file.

513
00:40:54,660 --> 00:40:55,920
OK, it is generating.

514
00:40:57,060 --> 00:41:00,840
You can see that it is 
loading LoRA from the selected 

515
00:41:01,620 --> 00:41:05,160
checkpoint from here and applying weight.

516
00:41:05,880 --> 00:41:08,340
As you can see here: LoRA weight:

517
00:41:08,340 --> 00:41:13,680
What percentage of LoRA weight should be applied
to the UNET when training or creating checkpoint.

518
00:41:13,680 --> 00:41:16,260
Applying the text weight as well.

519
00:41:16,860 --> 00:41:18,720
And then it is saving.

520
00:41:18,720 --> 00:41:22,080
However, the saved file
name is not correct.

521
00:41:22,080 --> 00:41:27,120
You see, it has appended the
latest training number,

522
00:41:27,120 --> 00:41:30,660
However it has loaded 2400.

523
00:41:30,660 --> 00:41:37,980
So I think there is a simple
mistake in the web UI.

524
00:41:37,980 --> 00:41:39,300
So where it is saved,

525
00:41:39,300 --> 00:41:42,780
It is saved inside stable 
diffusion installation folder, 

526
00:41:42,780 --> 00:41:46,980
then models, then stable diffusion folder.

527
00:41:46,980 --> 00:41:52,020
OK, I am going to rename this
into the name that I want.

528
00:41:52,020 --> 00:41:54,000
Let's say LoRA.

529
00:41:54,840 --> 00:41:59,340
And you see it also has YAML file
and it has to be the same name.

530
00:41:59,340 --> 00:42:01,380
OK, I did rename with F2.

531
00:42:02,100 --> 00:42:09,240
OK, now I can do text2img and generate
new images based on our training.

532
00:42:09,240 --> 00:42:10,620
How we are going to do that:

533
00:42:10,620 --> 00:42:17,460
First click refresh here and then
my new model has appeared here.

534
00:42:17,460 --> 00:42:18,660
It is now loading.

535
00:42:18,660 --> 00:42:22,260
You can see the loading from
this command window here:

536
00:42:22,260 --> 00:42:27,060
Loading config and loading
other parameters.

537
00:42:27,060 --> 00:42:29,580
OK, the model has been loaded.

538
00:42:29,580 --> 00:42:34,020
Now what is the prompt that
we are going to do?

539
00:42:34,020 --> 00:42:40,980
The prompt we are going to do is the prompt we
have given in here, which is my brother face.

540
00:42:40,980 --> 00:42:42,720
OK, this is our unique keyword.

541
00:42:43,380 --> 00:42:47,340
And then we will append the
other keywords that we want.

542
00:42:48,480 --> 00:42:53,880
Even though our model has learned
the face very good

543
00:42:53,880 --> 00:43:00,960
As soon as we add new keywords to 
improve and obtain different styles 

544
00:43:00,960 --> 00:43:05,520
of the learned face, it totally 
produces different images.

545
00:43:05,520 --> 00:43:13,680
Unfortunately, no matter how many times I
have tried, my all attempts have failed.

546
00:43:13,680 --> 00:43:18,480
It always produces different faces,
not the face it has learned.

547
00:43:18,480 --> 00:43:25,560
If I only give my prompt instance, yes,
it produces the face of my brother.

548
00:43:25,560 --> 00:43:28,560
But then what is the purpose of training?

549
00:43:28,560 --> 00:43:34,320
Because I am not able to modify it. Change
the style, produce different styles.

550
00:43:34,320 --> 00:43:39,780
Therefore, now I will do another
training with SD version 1.5.

551
00:43:39,780 --> 00:43:48,180
And let's see the difference between SD version
2.1 and 1.5 when we are doing face training.

552
00:43:49,380 --> 00:43:59,640
Since SD version 1.5 requires 512 pixel
resolution, I am recutting the images,

553
00:43:59,640 --> 00:44:02,580
as you can see. I am cropping them again

554
00:44:03,180 --> 00:44:06,780
And I have removed some
of the very old images.

555
00:44:06,780 --> 00:44:09,120
Actually, I only removed two of them.

556
00:44:09,120 --> 00:44:16,920
Okay, it is like this that I am setting up
the images for SD version 1.5 training.

557
00:44:16,920 --> 00:44:22,380
Okay, yes, like this.

558
00:44:22,380 --> 00:44:30,840
Save as zip and open the downloaded file
and extract them into the folder.

559
00:44:33,360 --> 00:44:38,580
Okay, here, like this, I will overwrite
and the training set is now ready.

560
00:44:38,580 --> 00:44:44,520
Okay, for 1.5, I am first
changing model into 1.5.

561
00:44:44,520 --> 00:44:46,740
Then I am going to the reboot.

562
00:44:46,740 --> 00:44:51,180
And in here we are going
to generate a new model.

563
00:44:51,180 --> 00:44:57,420
Let's say, okay, brother,
SD 15, like this.

564
00:44:57,420 --> 00:45:02,820
And the source checkpoint will be 1.5
because we are starting a new model.

565
00:45:02,820 --> 00:45:07,560
Let's generate the model like this:

566
00:45:07,560 --> 00:45:11,280
Okay, it is preparing the
model file for training.

567
00:45:12,060 --> 00:45:13,620
Actually, everything is same.

568
00:45:14,400 --> 00:45:17,100
Then I am clicking training wizard person.

569
00:45:17,100 --> 00:45:20,100
It will set the parameters.

570
00:45:20,100 --> 00:45:21,960
Oh, I think I click it.

571
00:45:21,960 --> 00:45:25,320
Yeah, I didn't wait process to finish.

572
00:45:26,400 --> 00:45:28,680
Okay, now I will set again.

573
00:45:28,680 --> 00:45:31,860
Okay, now it is set and
the model is also set.

574
00:45:32,580 --> 00:45:36,900
All right, you see, the model has
arrived here for training.

575
00:45:36,900 --> 00:45:39,720
Okay, I am just doing the same things.

576
00:45:39,720 --> 00:45:51,600
By the way, we now need to recompose new class
images for improving the test accuracy.

577
00:45:51,600 --> 00:45:56,880
Also, this apply: horizontal flip 
means that on the runtime it will 

578
00:45:56,880 --> 00:46:02,400
sometimes provide the horizontally 
flipped images in the training.

579
00:46:02,400 --> 00:46:05,520
Okay, it won't generate new
images on the folder,

580
00:46:05,520 --> 00:46:07,740
It will do that on the runtime.

581
00:46:07,740 --> 00:46:10,860
Okay, I am selecting LoRA.

582
00:46:10,860 --> 00:46:18,420
I am using 8bit adam with FP16, xFormers.
Don't Cache Latents.

583
00:46:25,620 --> 00:46:29,220
Okay, I am not changing other things 
because it is already learning good,  

584
00:46:29,220 --> 00:46:32,520
but we weren't able to generate good images.

585
00:46:32,520 --> 00:46:38,520
I think it was due to version,
SD version 2.1.

586
00:46:38,520 --> 00:46:46,620
Okay, the path for our VAE
here. And classification.

587
00:46:46,620 --> 00:46:53,700
We now need to make new classification,
so I will just make another folder too.

588
00:46:53,700 --> 00:46:56,520
Okay, like this:

589
00:46:56,520 --> 00:46:57,720
Let's enter here.

590
00:46:58,680 --> 00:47:03,060
All right, we are leaving this empty because
we are only teaching one phase.

591
00:47:04,500 --> 00:47:09,240
I will give the same name as the
model name to instance prompt.

592
00:47:09,240 --> 00:47:11,100
Now class prompt.

593
00:47:11,100 --> 00:47:15,120
This is important to decide class prompt.

594
00:47:15,120 --> 00:47:16,800
Now I will do a few tests here.

595
00:47:16,800 --> 00:47:23,820
Okay, with a simple prompt, such 
as face photo of a man: 8K HDR, 

596
00:47:23,820 --> 00:47:29,880
smooth, sharp focus and 
cinematography, we got decent faces.

597
00:47:29,880 --> 00:47:33,120
So this will be our class prompt.

598
00:47:33,120 --> 00:47:39,180
Okay, let's go back to our DreamBooth training,
and the class prompt will be like this:

599
00:47:39,180 --> 00:47:41,520
Classification image, negative prompt.

600
00:47:41,520 --> 00:47:45,060
So let's also copy and paste it.

601
00:47:45,060 --> 00:47:49,140
By the way, don't worry, I will
provide these as comments.

602
00:47:49,140 --> 00:47:53,700
Okay, so the sample image prompt
will be same as before.

603
00:47:53,700 --> 00:48:00,480
Okay, and should I provide negative
image prompt for sample?

604
00:48:00,480 --> 00:48:02,880
Yes, let's also provide it.

605
00:48:02,880 --> 00:48:05,520
Okay, how many we want?

606
00:48:05,520 --> 00:48:09,900
This time? We have how many
images in the folder?

607
00:48:09,900 --> 00:48:11,580
Let's check it out

608
00:48:12,720 --> 00:48:20,580
Once again. Okay, we have 14,
so I will generate just 140.

609
00:48:20,580 --> 00:48:24,540
Okay, I'm not touching this.

610
00:48:24,540 --> 00:48:26,580
Parameters are set.

611
00:48:26,580 --> 00:48:28,560
Everything looking good.

612
00:48:28,560 --> 00:48:31,860
Okay, let's start another training.

613
00:48:35,700 --> 00:48:40,440
So you see, since we don't 
have any concept images now, it 

614
00:48:40,440 --> 00:48:44,820
is going to generate first 
our class images, as before.

615
00:48:44,820 --> 00:48:52,020
But this time we are using 512 pixel 
resolution. This is really important because 

616
00:48:52,020 --> 00:48:59,400
our base model is now version 1.5 and it 
is using 512 pixel as native resolution.

617
00:49:00,600 --> 00:49:11,100
Generating class images are now much faster, you
see, because it is now lower size in dimension.

618
00:49:12,660 --> 00:49:17,520
Okay so, the classification training 
set has been completed and now 

619
00:49:17,520 --> 00:49:24,180
the training has started and so 
far we are at the 50 training step.

620
00:49:24,180 --> 00:49:29,160
You see, it is much faster now 
than before because we are simply 

621
00:49:29,160 --> 00:49:38,640
working with 0.44, which means 
44% of image size than before.

622
00:49:38,640 --> 00:49:43,200
Because before we were
working on 768 pixels.

623
00:49:43,200 --> 00:49:46,260
Now we are working with 512 pixels.

624
00:49:46,260 --> 00:49:49,440
Therefore, it is more than
two times faster.

625
00:49:50,760 --> 00:49:58,020
New model training checkpoints are also getting
saved under the models LoRA folder.

626
00:49:58,020 --> 00:50:00,780
As you can see here with the
name that I have given.

627
00:50:01,680 --> 00:50:05,340
Also, the new folder under 
DreamBooth has been generated 

628
00:50:05,340 --> 00:50:10,200
for the new training in here, brother SD15.

629
00:50:10,200 --> 00:50:13,920
And in here we can see the 
samples it is generating: 

630
00:50:13,920 --> 00:50:16,500
so far, nothing resembling at all.

631
00:50:17,220 --> 00:50:19,620
Okay so the training has been completed.

632
00:50:19,620 --> 00:50:23,460
I let it run during the 
night while I was sleeping, 

633
00:50:23,460 --> 00:50:25,440
so generated so many checkpoints.

634
00:50:25,440 --> 00:50:29,520
And now I am going to use 
this particular checkpoint 

635
00:50:29,520 --> 00:50:36,480
to generate our .ckpt file from here.

636
00:50:36,480 --> 00:50:38,700
As usual, as previously same.

637
00:50:38,700 --> 00:50:44,280
I have selected the model 
entered, selected the checkpoint, 

638
00:50:44,280 --> 00:50:49,440
given a name, and then 
click it generate ckpt file.

639
00:50:49,440 --> 00:50:55,500
Then now I am going to load
newly generated ckpt file.

640
00:50:55,500 --> 00:50:57,660
So to do that, just click refresh.

641
00:50:58,740 --> 00:50:59,880
And it should come.

642
00:50:59,880 --> 00:51:01,680
And yes, it is.

643
00:51:01,680 --> 00:51:05,280
It has arrived. With now checkpointing
that model.

644
00:51:05,280 --> 00:51:06,060
It is done.

645
00:51:06,060 --> 00:51:09,840
And now we can do our tests.

646
00:51:10,500 --> 00:51:15,000
OK, I have generated over 
600 images and some of them 

647
00:51:15,000 --> 00:51:18,420
are really good and really resembling the face

648
00:51:18,420 --> 00:51:20,100
we teach it.

649
00:51:20,100 --> 00:51:25,140
So the key thing is that you 
need to generate more images with 

650
00:51:25,140 --> 00:51:29,760
LoRA because I think it is 
not as precise as DreamBooth.

651
00:51:29,760 --> 00:51:31,140
The prompt I  

652
00:51:31,140 --> 00:51:39,120
have used is portrait photo of brother SD
15, which is my prompt instance, with weight 1.2.

653
00:51:39,840 --> 00:51:46,560
1.2 weight means that it will give 
more importance to this keyword. On the 

654
00:51:46,560 --> 00:51:51,420
official page of automatic 1111 
stable diffusion web UI wiki features,

655
00:51:51,420 --> 00:51:54,780
You can see attention 
emphasis and it is explaining 

656
00:51:55,440 --> 00:51:58,740
that how you can give more attention to each word.

657
00:51:58,740 --> 00:52:05,040
You can also use parentheses like this, or
you can directly set importance like this.

658
00:52:05,040 --> 00:52:09,120
So it is totally up to you
to use the either way.

659
00:52:09,120 --> 00:52:13,200
So I have given more importance 
to the prompt instance 

660
00:52:13,200 --> 00:52:16,860
and I have also written photo of brother SD 15.

661
00:52:16,860 --> 00:52:21,540
And then I have used a 
generic keywords to generate 

662
00:52:21,540 --> 00:52:24,840
images as close as to our prompt instance.

663
00:52:24,840 --> 00:52:29,520
You see 8K HDR, smooth,
sharp focus cinematic.

664
00:52:29,520 --> 00:52:34,680
I am going to share all these 
keywords in the comments of the 

665
00:52:34,680 --> 00:52:41,160
video, and I have also entered 
a lengthy negative prompt.

666
00:52:41,160 --> 00:52:46,620
I have used Eular a as a 
sampling method with 25 steps 

667
00:52:46,620 --> 00:52:51,840
and the native resolution for SD 1.5 512 pixels.

668
00:52:51,840 --> 00:52:55,680
So how can you generate
more than 100 images?

669
00:52:55,680 --> 00:53:00,540
Set the batch count to 100,
then go to bottom. Here.

670
00:53:00,540 --> 00:53:04,920
You will see the script
section. By default

671
00:53:04,920 --> 00:53:08,700
it is set to none, but you 
can go to prompts from file or 

672
00:53:08,700 --> 00:53:12,600
text box and you can just 
copy and paste your prompt.

673
00:53:12,600 --> 00:53:18,420
So it will read each line and 
will continue generating images 

674
00:53:18,420 --> 00:53:21,060
until all of the lines are executed.

675
00:53:21,060 --> 00:53:26,700
With this way, you can generate
much more images.

676
00:53:26,700 --> 00:53:30,360
Also, there are other options
that you can do here.

677
00:53:30,360 --> 00:53:33,600
For example, you can do X and Y plots.

678
00:53:33,600 --> 00:53:38,280
So you can give X values and 
Y values and, if you wonder 

679
00:53:38,280 --> 00:53:41,520
what they are, separate values 
for X axis, using commas.

680
00:53:42,960 --> 00:53:51,660
You can play with these to, for example, generate 
different style images with having, let's say, 

681
00:53:52,380 --> 00:54:01,680
artist names or style names in your X values and 
the Y values would be like your regular prompt.

682
00:54:02,820 --> 00:54:08,760
Okay, so I have selected few of the images and
now I will show you how to upscale them.

683
00:54:08,760 --> 00:54:12,900
The resemblance rate is not as good
as DreamBooth, unfortunately.

684
00:54:12,900 --> 00:54:16,920
So you can also do DreamBooth training.

685
00:54:16,920 --> 00:54:22,020
The only thing different in DreamBooth 
training than the LoRA is in the advanced 

686
00:54:22,020 --> 00:54:26,400
setup. You just don't pick this LoRA 
and it will do DreamBooth training.

687
00:54:26,400 --> 00:54:30,060
And also be careful that when you 
are DreamBooth training it will 

688
00:54:30,060 --> 00:54:35,460
generate 5GB files, 4GB files, 
at the each save checkpoint.

689
00:54:35,460 --> 00:54:43,800
So you may want to reduce this, increase the save
checkpoint frequency, not just 10, but maybe 50.

690
00:54:43,800 --> 00:54:47,040
It totally depends on your
hardware hard drive.

691
00:54:47,040 --> 00:54:52,680
Okay, so what am I going to do is first
let's check out the PNG info.

692
00:54:53,220 --> 00:54:57,360
Tap in here in pictures and
in brother selected.

693
00:54:57,360 --> 00:54:58,380
Let's pick one of them.

694
00:54:58,380 --> 00:55:04,020
So you see Web UI embeds the meta
information of the parameters.

695
00:55:04,020 --> 00:55:09,600
So if you can get the original image 
that is generated by the Web UI, 

696
00:55:09,600 --> 00:55:15,960
you can just use PNG info to extract 
the parameters from that image.

697
00:55:15,960 --> 00:55:19,140
And the one another thing I am
going to show you is extras.

698
00:55:19,140 --> 00:55:23,220
In extras, let's first
try a single image.

699
00:55:23,880 --> 00:55:25,680
You can upscale it.

700
00:55:25,680 --> 00:55:31,740
Okay, the best upscaling algorithm
I have found is R-ESRGAN 4x+.

701
00:55:31,740 --> 00:55:37,980
I pretty much like this. And let's
upscale to 3X dimension.

702
00:55:38,820 --> 00:55:42,060
The first time you do. It may
download something in here.

703
00:55:43,020 --> 00:55:46,980
Since I have done it previously, it didn't
download the necessary models.

704
00:55:47,580 --> 00:55:49,560
Okay, the upscaling is done.

705
00:55:49,560 --> 00:55:52,620
As you can see, now this
is upscaled version.

706
00:55:52,620 --> 00:55:57,469
You can also apply GFPGAN visibility.

707
00:55:57,469 --> 00:56:03,840
The GFPGAN will improve
the face of a human.

708
00:56:03,840 --> 00:56:05,280
It is another model.

709
00:56:05,280 --> 00:56:07,380
Let's do that and see the difference.

710
00:56:10,560 --> 00:56:12,840
Okay, it is getting done.

711
00:56:13,800 --> 00:56:18,720
And yes, so now you see it
is more like a real human.

712
00:56:18,720 --> 00:56:25,620
This is fixing eyes much better, 
making the eyes much better

713
00:56:25,620 --> 00:56:28,440
if they are not oriented, if
they are not symmetric.

714
00:56:29,760 --> 00:56:33,720
So you may want to apply this
as well, if you want.

715
00:56:34,860 --> 00:56:37,740
And also you can do batch processing.

716
00:56:37,740 --> 00:56:43,140
For batch processing, just open the folder
with Ctrl-A, select all, open.

717
00:56:43,140 --> 00:56:45,720
It will be loaded like this.

718
00:56:45,720 --> 00:56:50,520
Then it will apply all of the parameters
you set here and click generate.

719
00:56:51,300 --> 00:56:56,700
And all of the images will
be generated as a batch.

720
00:56:57,600 --> 00:57:01,560
So the results of the batch generation
will be saved in a folder.

721
00:57:01,560 --> 00:57:04,680
If you want to open that folder, just
click this folder image here.

722
00:57:04,680 --> 00:57:10,140
You see open images output directory and the
batch processing results will appear here.

723
00:57:10,140 --> 00:57:16,080
You can just directly copy them, paste
them and do whatever you want.

724
00:57:17,460 --> 00:57:19,140
Okay, this is all for today.

725
00:57:19,140 --> 00:57:24,240
Please ask any questions that you might
have. To improve the performance

726
00:57:24,240 --> 00:57:31,500
so I suggest you to use DreamBooth
instead of the LoRA training.

727
00:57:31,500 --> 00:57:35,220
Also, you can improve your data set.

728
00:57:35,220 --> 00:57:39,660
The data set, our data set,
was not that very good.

729
00:57:39,660 --> 00:57:44,820
You see almost same time captured,
same pose images.

730
00:57:44,820 --> 00:57:51,960
So if you add more variety to your data set,
you will obtain better results, more likely.

731
00:57:51,960 --> 00:57:57,660
And also, please like, share and subscribe
our channel if you have enjoyed.

732
00:57:57,660 --> 00:58:01,440
And if you support us on Patreon
we would appreciate very much.

733
00:58:01,440 --> 00:58:05,520
Currently, so far, we have one
Patreon, as you can see.

734
00:58:05,520 --> 00:58:08,700
Thank you very much to our beloved
Patreon, by the way,

735
00:58:08,700 --> 00:58:11,460
And I am hoping that you
will support us as well.

736
00:58:11,460 --> 00:58:17,400
Hopefully, more videos, more advanced videos
will come for Stable Diffusion.

737
00:58:18,540 --> 00:58:23,880
If you want to also learn something about
stable diffusion, let me know by comments.

738
00:58:23,880 --> 00:58:26,940
And hopefully I will make
videos about them.

739
00:58:26,940 --> 00:58:29,580
Hopefully, see you in another video.

